{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fi1bf4mzWAt2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import h5py\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TuSZdTlziXf_"
      },
      "outputs": [],
      "source": [
        "def load_pairs_from_hdf5(hdf5_file_path, hdf5_folder):\n",
        "    with h5py.File(hdf5_file_path, 'r') as hdf:\n",
        "        loaded_pairs = []\n",
        "\n",
        "        # Function to convert a single relative path back to an absolute path\n",
        "        def make_absolute(rel_path):\n",
        "            # Decode if the path is a byte string\n",
        "            if isinstance(rel_path, bytes):\n",
        "                rel_path = rel_path.decode('utf-8')\n",
        "            parts = rel_path.split('/')\n",
        "            new_parts = []\n",
        "            for part in parts:\n",
        "              if part == 'sun3d_extracted' or part =='..':\n",
        "                continue\n",
        "              new_parts.append(part)\n",
        "            corrected_path = '/'.join(new_parts)\n",
        "            absolute_path = os.path.join(hdf5_folder, corrected_path)\n",
        "            return absolute_path\n",
        "            # rel_path = rel_path.replace('../sun3d_extracted', '')\n",
        "            # return os.path.join(hdf5_folder, rel_path)\n",
        "\n",
        "        # Function to process paths in pairs\n",
        "        def process_paths(img_paths_array):\n",
        "            # Ensure each path in the tuple is absolute\n",
        "            return tuple(make_absolute(path) for path in img_paths_array)\n",
        "\n",
        "        # Load pairs\n",
        "        pairs_group = hdf['pairs']\n",
        "        for pair_name in pairs_group:\n",
        "            pair_group = pairs_group[pair_name]\n",
        "            img_paths_array = pair_group['img_paths'][()]  # This will be a NumPy array\n",
        "            img_paths = process_paths(img_paths_array)  # Process each path to be absolute\n",
        "            points1 = torch.tensor(pair_group['points1'][()])\n",
        "            pos_points2 = torch.tensor(pair_group['pos_points2'][()])\n",
        "            neg_points2 = torch.tensor(pair_group['neg_points2'][()])\n",
        "            loaded_pairs.append({\n",
        "                'img_paths': img_paths,\n",
        "                'points1': points1,\n",
        "                'pos_points2': pos_points2,\n",
        "                'neg_points2': neg_points2\n",
        "            })\n",
        "\n",
        "    return loaded_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jOjFotKWiXgA"
      },
      "outputs": [],
      "source": [
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "output_path = os.path.join(current_directory, os.pardir,  os.pardir,'datasets','sun3d_training')\n",
        "# output_path = '/content/drive/MyDrive/project_slam/dataset/sun3d_training'\n",
        "hdf5_file_path = os.path.join(output_path, 'pairs.hdf5')\n",
        "# hdf5_file_path = '/content/drive/MyDrive/project_slam/dataset/sun3d_training/pairs.hdf5'\n",
        "loaded_pairs= load_pairs_from_hdf5(hdf5_file_path,output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yiUhO8xras1m"
      },
      "outputs": [],
      "source": [
        "class BinarizedActivation(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(torch.sign(input))\n",
        "        return torch.sign(input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        # grad_input[(input < -1) | (input > 1)] = 0\n",
        "        grad_input[(input.abs() > 1)] = 0\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "class GCNv2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNv2, self).__init__()\n",
        "        # self.elu = F.elu\n",
        "        self.elu = torch.nn.ELU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.convF_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.convF_2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.convD_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.convD_2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.binarized_activation = BinarizedActivation()\n",
        "        self.pixel_shuffle = nn.PixelShuffle(16)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.elu(self.conv1(x))\n",
        "        x = self.elu(self.conv2(x))\n",
        "        x = self.elu(self.conv3_1(x))\n",
        "        x = self.elu(self.conv3_2(x))\n",
        "        x = self.elu(self.conv4_1(x))\n",
        "        x = self.elu(self.conv4_2(x))\n",
        "\n",
        "        # Descriptor\n",
        "        xF = self.elu(self.convF_1(x))\n",
        "        desc = self.convF_2(xF)\n",
        "        dn = torch.norm(desc, p=2, dim=1)\n",
        "        # desc = BinarizedActivation.apply(desc.div(torch.unsqueeze(dn, 1)))\n",
        "        desc = desc.div(torch.unsqueeze(dn, 1))\n",
        "        desc = self.binarized_activation.apply(desc)\n",
        "        # Detector\n",
        "        xD = self.elu(self.convD_1(x))\n",
        "        det = self.convD_2(xD).sigmoid()\n",
        "        det = self.pixel_shuffle(det)\n",
        "        return desc, det\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kPEaQQhQ7wdg"
      },
      "outputs": [],
      "source": [
        "class loss_calculator_3:\n",
        "    def __init__(self, model):\n",
        "        self.model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def remove_padding(self, pts):\n",
        "        mask = (pts != torch.tensor([float('inf'), float('inf')]))\n",
        "        non_padding_vectors = pts[mask.all(dim=1)]\n",
        "        return non_padding_vectors\n",
        "\n",
        "    def get_images_for_batch(self, img_paths):\n",
        "      images = []\n",
        "      # img_paths = batch_loaded_pairs['img_paths']\n",
        "      for img_path in img_paths:\n",
        "          img1, img2 = cv2.imread(img_path[0]), cv2.imread(img_path[1])\n",
        "          gray_img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "          gray_img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "          img_tensor1 = torch.from_numpy(gray_img1).unsqueeze(0).float().to(self.device)\n",
        "          img_tensor2 = torch.from_numpy(gray_img2).unsqueeze(0).float().to(self.device)\n",
        "          images.append((img_tensor1, img_tensor2))\n",
        "\n",
        "      return images\n",
        "\n",
        "    def normalized_pts(self, batch, height=480, width=640):\n",
        "      normalized_pts = {}\n",
        "      scale_w = float(width) / 2.\n",
        "      scale_h = float(height) / 2.\n",
        "      pts_cur = batch['points1'].clone()\n",
        "      pts_tar_pos = batch['pos_points2'].clone()\n",
        "      pts_tar_neg = batch['neg_points2'].clone()\n",
        "\n",
        "      normalized_pts['cur'] = torch.zeros_like(pts_cur)\n",
        "      normalized_pts['tar_pos'] = torch.zeros_like(pts_tar_pos)\n",
        "      normalized_pts['tar_neg'] = torch.zeros_like(pts_tar_neg)\n",
        "      normalized_pts['cur'][:,:,0] = pts_cur[:,:,0] / scale_w - 1\n",
        "      normalized_pts['cur'][:,:,1] = pts_cur[:,:,1] / scale_h - 1\n",
        "      normalized_pts['tar_pos'][:,:,0] = pts_tar_pos[:,:,0] /scale_w - 1\n",
        "      normalized_pts['tar_pos'][:,:,1] = pts_tar_pos[:,:,1] / scale_h - 1\n",
        "      normalized_pts['tar_neg'][:,:,0] = pts_tar_neg[:,:,0] / scale_w - 1\n",
        "      normalized_pts['tar_neg'][:,:,1] = pts_tar_neg[:,:,1] / scale_h - 1\n",
        "      # for key in normalized_pts:\n",
        "      #     if torch.isinf(normalized_pts[key]).any():\n",
        "      #         normalized_pts[key] = self.remove_padding(normalized_pts[key])\n",
        "      return normalized_pts\n",
        "\n",
        "    def get_desc_pairs(self, batch, height=480, width=640):\n",
        "      batchsize= len(batch.get('img_paths'))\n",
        "      normal_pts = self.normalized_pts(batch, height, width)\n",
        "      img_paths = batch.get('img_paths')\n",
        "      gray_images = self.get_images_for_batch(img_paths) #### get img_paths\n",
        "      desc_dict = {}\n",
        "      for key, pts_group in normal_pts.items():\n",
        "        desc = []\n",
        "        for i in range(batchsize):\n",
        "          inp_cur, inp_tar =  gray_images[i]\n",
        "          inp_cur = inp_cur.unsqueeze(0).float()\n",
        "          inp_tar = inp_tar.unsqueeze(0).float()\n",
        "          desc_cur, desc_tar = self.model(inp_cur)[0], self.model(inp_tar)[0]\n",
        "          desc_cur.requires_grad_(True)\n",
        "          desc_tar.requires_grad_(True)\n",
        "          desc_tensor = desc_cur if key == 'cur' else desc_tar\n",
        "          if torch.isinf(pts_group[i]).any():\n",
        "              pts = self.remove_padding(pts_group[i]).view(1, 1, -1, 2).float().to(self.device)\n",
        "          else:\n",
        "            pts = pts_group[i].view(1, 1, -1, 2).float().to(self.device)\n",
        "          sample_desc = F.grid_sample(desc_tensor, pts, align_corners=False, mode='nearest').squeeze()\n",
        "          desc.append(sample_desc)\n",
        "\n",
        "        desc_dict[key] = desc\n",
        "      return desc_dict\n",
        "    \n",
        "    def hamming_distance(self,tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Calculate the Hamming distance between two tensors.\n",
        "\n",
        "        Args:\n",
        "            tensor1 (torch.Tensor): The first tensor.\n",
        "            tensor2 (torch.Tensor): The second tensor, must be the same shape as tensor1.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor of Hamming distances.\n",
        "        \"\"\"\n",
        "        if tensor1.dtype != torch.bool:\n",
        "            tensor1 = tensor1.bool()\n",
        "        if tensor2.dtype != torch.bool:\n",
        "            tensor2 = tensor2.bool()\n",
        "        \n",
        "        differing_bits = tensor1 ^ tensor2\n",
        "        distance = differing_bits.sum(dim=-1)\n",
        "        return distance\n",
        "\n",
        "    def batch_l_desc_loss(self, desc_batch, margin=1.0):\n",
        "      ldesc = 0\n",
        "      for cur_list, pos_list, neg_list in zip(desc_batch['cur'], desc_batch['tar_pos'], desc_batch['tar_neg']):\n",
        "          # print(\"Shapes:\", cur_list.shape, pos_list.shape, neg_list.shape)\n",
        "          # pairwise_dist_pos = torch.sum((cur_list - pos_list) ** 2, dim=-1)\n",
        "          # pairwise_dist_neg = torch.sum((cur_list - neg_list) ** 2, dim=-1)\n",
        "          \n",
        "          # Compute Hamming distances\n",
        "          pairwise_dist_pos = self.hamming_distance(cur_list, pos_list)\n",
        "          pairwise_dist_neg = self.hamming_distance(cur_list, neg_list)\n",
        "\n",
        "          sample_loss = torch.sum(torch.max(torch.zeros_like(pairwise_dist_pos), pairwise_dist_pos - pairwise_dist_neg + margin))\n",
        "          ldesc += sample_loss\n",
        "      return ldesc.to(self.device)\n",
        "\n",
        "    def get_det_pairs(self, batch):\n",
        "      img_paths = batch.get('img_paths')\n",
        "      batchsize= len(img_paths)\n",
        "      gray_images = self.get_images_for_batch(img_paths)\n",
        "      det_dict = {}\n",
        "      det_cur_list = []\n",
        "      det_tar_list = []\n",
        "      for i in range(batchsize):\n",
        "          inp_cur, inp_tar =  gray_images[i]\n",
        "          inp_cur = inp_cur.unsqueeze(0).float()\n",
        "          inp_tar = inp_tar.unsqueeze(0).float()\n",
        "          det_cur, det_tar = self.model(inp_cur)[1], self.model(inp_tar)[1]\n",
        "          det_cur.requires_grad_(True)\n",
        "          det_tar.requires_grad_(True)\n",
        "          det_cur_list.append(det_cur.squeeze())\n",
        "          det_tar_list.append(det_tar.squeeze())\n",
        "      det_cur_list = torch.stack(det_cur_list, dim=0)\n",
        "      det_tar_list = torch.stack(det_tar_list, dim=0)\n",
        "      det_dict = {'points1': det_cur_list, 'pos_points2': det_tar_list}\n",
        "      return det_dict\n",
        "\n",
        "    def generate_batch_mask(self, coords, height=480, width=640):\n",
        "\n",
        "      mask = torch.zeros((coords.shape[0], height, width), dtype=torch.uint8)\n",
        "      for batch_idx in range(coords.shape[0]):\n",
        "          batch_coords = coords[batch_idx]\n",
        "          for point_idx, (x, y) in enumerate(batch_coords):\n",
        "              x = x.int()\n",
        "              y = y.int()\n",
        "              if 0 <= x < width and 0 <= y < height:\n",
        "                  mask[batch_idx, y, x] = 1\n",
        "\n",
        "      return mask\n",
        "\n",
        "\n",
        "    def transform_target(self,batch,height=480,width=640):\n",
        "        img_path_cur = batch.get('img_paths')[0]\n",
        "        img_path_tar = batch.get('img_paths')[1]\n",
        "        coord_cur = batch.get('points1')\n",
        "        coord_tar = batch.get('pos_points2')\n",
        "        cur_tensor = self.generate_batch_mask(coord_cur,height,width)\n",
        "        tar_tensor = self.generate_batch_mask(coord_tar,height,width)\n",
        "        return cur_tensor, tar_tensor\n",
        "\n",
        "    def l_det_loss(self, o_cur, c_cur, o_tar, c_tar, alpha1=1.0, alpha2=1.0):\n",
        "        Lce_cur = self.binary_cross_entropy(o_cur, c_cur)\n",
        "        Lce_tar = self.binary_cross_entropy(o_tar, c_tar)\n",
        "        Ldet = alpha1 * Lce_cur + alpha2 * Lce_tar\n",
        "        return Ldet\n",
        "    def binary_cross_entropy(self, o, c):\n",
        "        c = c.to(o.dtype)\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(o, c, reduction='none')\n",
        "        total_loss = torch.sum(bce_loss)\n",
        "        return total_loss\n",
        "\n",
        "    def batch_l_det_loss(self, batch, det_batch, margin=1.0):\n",
        "        ldet = 0\n",
        "        # for i in range(len(batch_loaded_pairs)):\n",
        "        cur_det = det_batch['points1']\n",
        "        tar_det = det_batch['pos_points2']\n",
        "        trans_cur = self.transform_target(batch,height=480,width=640)[0].to(self.device)\n",
        "        trans_tar_pos = self.transform_target(batch,height=480,width=640)[1].to(self.device)\n",
        "        # trans_cur = batch_loaded_pairs[i]['trans_cur']\n",
        "        # trans_tar_pos = batch_loaded_pairs[i]['trans_tar_pos']\n",
        "        ldet += self.l_det_loss(cur_det, trans_cur, tar_det, trans_tar_pos)\n",
        "        return ldet\n",
        "\n",
        "\n",
        "    def loss(self, batch_loaded_pairs, batch_size, height=480, width=640, margin=1.0):\n",
        "\n",
        "      desc_batch = self.get_desc_pairs(batch_loaded_pairs, height, width)\n",
        "      det_batch = self.get_det_pairs(batch_loaded_pairs)\n",
        "      # for key, desc_list in desc_batch.items():\n",
        "      #   for desc_tensor in desc_list:\n",
        "      #       print(f\"Tensor name: {key}, requires_grad: {desc_tensor.requires_grad}\")\n",
        "      loss_desc = self.batch_l_desc_loss(desc_batch, margin)\n",
        "      # loss_desc = loss_desc.requires_grad_(True)\n",
        "      loss_det = self.batch_l_det_loss(batch_loaded_pairs, det_batch, margin)\n",
        "      # loss_det = loss_det.requires_grad_(True)\n",
        "      return (loss_desc +  0.001*loss_det)/batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ng7ABLtJKYBT"
      },
      "outputs": [],
      "source": [
        "model = GCNv2()\n",
        "loss_calculator = loss_calculator_3(model)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_len_points = max(len(sample['points1']) for sample in batch)\n",
        "    for sample in batch:\n",
        "      n = max_len_points - len(sample['points1'])\n",
        "      inf_tensor = torch.tensor([[float('inf'), float('inf')]] *n )\n",
        "      sample['points1'] = torch.cat((sample['points1'],inf_tensor), dim=0)\n",
        "      sample['pos_points2'] = torch.cat((sample['pos_points2'], inf_tensor), dim=0)\n",
        "      sample['neg_points2'] = torch.cat((sample['neg_points2'], inf_tensor), dim=0)\n",
        "    img_path = [sample['img_paths'] for sample in batch]\n",
        "    points1 = torch.stack([sample['points1'] for sample in batch], dim=0)\n",
        "    pos_points2 = torch.stack([sample['pos_points2'] for sample in batch], dim=0)\n",
        "    neg_points2 = torch.stack([sample['neg_points2'] for sample in batch], dim=0)\n",
        "\n",
        "    return {'img_paths':img_path,'points1': points1, 'pos_points2': pos_points2, 'neg_points2': neg_points2}\n",
        "class Pairs(Dataset):\n",
        "    def __init__(self, loaded_pairs):\n",
        "        self.loaded_pairs = loaded_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.loaded_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.loaded_pairs[idx]\n",
        "        # path_tensor = torch.tensor(pair['img_paths'])\n",
        "        pts_tensor = pair['points1']\n",
        "        pos_tensor = pair['pos_points2']\n",
        "        neg_tensor = pair['neg_points2']\n",
        "        return {'img_paths':pair['img_paths'],'points1':pts_tensor,'pos_points2':pos_tensor,'neg_points2':neg_tensor}\n",
        "\n",
        "dataset = loaded_pairs\n",
        "\n",
        "batch_size = 4\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "# for batch in dataloader:\n",
        "#   l_total = loss_calculator.loss(batch,batch_size)\n",
        "#   print(l_total)\n",
        "  # print(count_ones)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer: torch.optim.Optimizer, epoch: int) -> None:\n",
        "    \"\"\"Halves the learning rate of the optimizer every 40 epochs.\n",
        "\n",
        "    Args:\n",
        "    optimizer (torch.optim.Optimizer): The optimizer for which to adjust the learning rate.\n",
        "    epoch (int): The current epoch number.\n",
        "\n",
        "    \"\"\"\n",
        "    # Every 40 epochs, halve the learning rate\n",
        "    if epoch % 40 == 0:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "vQIW2PjkqVwT",
        "outputId": "a27380a5-df38-418c-c450-622992a5fd7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [100/18689], Loss: 689.3131713867188\n",
            "Epoch [1/10], Step [200/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [300/18689], Loss: 689.309814453125\n",
            "Epoch [1/10], Step [400/18689], Loss: 689.3075561523438\n",
            "Epoch [1/10], Step [500/18689], Loss: 689.3075561523438\n",
            "Epoch [1/10], Step [600/18689], Loss: 689.3062744140625\n",
            "Epoch [1/10], Step [700/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [800/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [900/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [1000/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [1100/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [1200/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [1300/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [1400/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [1500/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [1600/18689], Loss: 689.309814453125\n",
            "Epoch [1/10], Step [1700/18689], Loss: 689.3070068359375\n",
            "Epoch [1/10], Step [1800/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [1900/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [2000/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [2100/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [2200/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [2300/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [2400/18689], Loss: 689.3073120117188\n",
            "Epoch [1/10], Step [2500/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [2600/18689], Loss: 689.3075561523438\n",
            "Epoch [1/10], Step [2700/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [2800/18689], Loss: 689.309814453125\n",
            "Epoch [1/10], Step [2900/18689], Loss: 689.31005859375\n",
            "Epoch [1/10], Step [3000/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [3100/18689], Loss: 689.3067626953125\n",
            "Epoch [1/10], Step [3200/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [3300/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [3400/18689], Loss: 689.309814453125\n",
            "Epoch [1/10], Step [3500/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [3600/18689], Loss: 689.309814453125\n",
            "Epoch [1/10], Step [3700/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [3800/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [3900/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [4000/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [4100/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [4200/18689], Loss: 689.309814453125\n",
            "Epoch [1/10], Step [4300/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [4400/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [4500/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [4600/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [4700/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [4800/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [4900/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [5000/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [5100/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [5200/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [5300/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [5400/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [5500/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [5600/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [5700/18689], Loss: 689.309814453125\n",
            "Epoch [1/10], Step [5800/18689], Loss: 689.31005859375\n",
            "Epoch [1/10], Step [5900/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [6000/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [6100/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [6200/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [6300/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [6400/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [6500/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [6600/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [6700/18689], Loss: 689.3075561523438\n",
            "Epoch [1/10], Step [6800/18689], Loss: 689.3073120117188\n",
            "Epoch [1/10], Step [6900/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [7000/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [7100/18689], Loss: 689.3070068359375\n",
            "Epoch [1/10], Step [7200/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [7300/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [7400/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [7500/18689], Loss: 689.3075561523438\n",
            "Epoch [1/10], Step [7600/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [7700/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [7800/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [7900/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [8000/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [8100/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [8200/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [8300/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [8400/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [8500/18689], Loss: 689.310546875\n",
            "Epoch [1/10], Step [8600/18689], Loss: 689.3073120117188\n",
            "Epoch [1/10], Step [8700/18689], Loss: 689.310302734375\n",
            "Epoch [1/10], Step [8800/18689], Loss: 689.3052978515625\n",
            "Epoch [1/10], Step [8900/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [9000/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [9100/18689], Loss: 689.3073120117188\n",
            "Epoch [1/10], Step [9200/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [9300/18689], Loss: 689.3062744140625\n",
            "Epoch [1/10], Step [9400/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [9500/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [9600/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [9700/18689], Loss: 689.3062744140625\n",
            "Epoch [1/10], Step [9800/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [9900/18689], Loss: 689.31005859375\n",
            "Epoch [1/10], Step [10000/18689], Loss: 689.3067626953125\n",
            "Epoch [1/10], Step [10100/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [10200/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [10300/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [10400/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [10500/18689], Loss: 689.31005859375\n",
            "Epoch [1/10], Step [10600/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [10700/18689], Loss: 689.3075561523438\n",
            "Epoch [1/10], Step [10800/18689], Loss: 689.31005859375\n",
            "Epoch [1/10], Step [10900/18689], Loss: 689.3073120117188\n",
            "Epoch [1/10], Step [11000/18689], Loss: 689.3073120117188\n",
            "Epoch [1/10], Step [11100/18689], Loss: 689.3070068359375\n",
            "Epoch [1/10], Step [11200/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [11300/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [11400/18689], Loss: 689.3062744140625\n",
            "Epoch [1/10], Step [11500/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [11600/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [11700/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [11800/18689], Loss: 689.3095703125\n",
            "Epoch [1/10], Step [11900/18689], Loss: 689.309814453125\n",
            "Epoch [1/10], Step [12000/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [12100/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [12200/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [12300/18689], Loss: 689.3073120117188\n",
            "Epoch [1/10], Step [12400/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [12500/18689], Loss: 689.3070068359375\n",
            "Epoch [1/10], Step [12600/18689], Loss: 689.3075561523438\n",
            "Epoch [1/10], Step [12700/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [12800/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [12900/18689], Loss: 689.3075561523438\n",
            "Epoch [1/10], Step [13000/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [13100/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [13200/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [13300/18689], Loss: 689.3075561523438\n",
            "Epoch [1/10], Step [13400/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [13500/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [13600/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [13700/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [13800/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [13900/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [14000/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [14100/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [14200/18689], Loss: 689.310302734375\n",
            "Epoch [1/10], Step [14300/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [14400/18689], Loss: 689.3067626953125\n",
            "Epoch [1/10], Step [14500/18689], Loss: 689.3050537109375\n",
            "Epoch [1/10], Step [14600/18689], Loss: 689.3073120117188\n",
            "Epoch [1/10], Step [14700/18689], Loss: 689.3073120117188\n",
            "Epoch [1/10], Step [14800/18689], Loss: 689.30859375\n",
            "Epoch [1/10], Step [14900/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [15000/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [15100/18689], Loss: 689.3073120117188\n",
            "Epoch [1/10], Step [15200/18689], Loss: 689.3070068359375\n",
            "Epoch [1/10], Step [15300/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [15400/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [15500/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [15600/18689], Loss: 689.3070068359375\n",
            "Epoch [1/10], Step [15700/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [15800/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [15900/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [16000/18689], Loss: 689.3082885742188\n",
            "Epoch [1/10], Step [16100/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [16200/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [16300/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [16400/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [16500/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [16600/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [16700/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [16800/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [16900/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [17000/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [17100/18689], Loss: 689.3067626953125\n",
            "Epoch [1/10], Step [17200/18689], Loss: 689.3065185546875\n",
            "Epoch [1/10], Step [17300/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [17400/18689], Loss: 689.30908203125\n",
            "Epoch [1/10], Step [17500/18689], Loss: 689.3080444335938\n",
            "Epoch [1/10], Step [17600/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [17700/18689], Loss: 689.3078002929688\n",
            "Epoch [1/10], Step [17800/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [17900/18689], Loss: 689.3070068359375\n",
            "Epoch [1/10], Step [18000/18689], Loss: 689.309326171875\n",
            "Epoch [1/10], Step [18100/18689], Loss: 689.308837890625\n",
            "Epoch [1/10], Step [18200/18689], Loss: 689.3082885742188\n"
          ]
        }
      ],
      "source": [
        "# for name, param in model.named_parameters():\n",
        "#     if 'weight' in name:  # 只对权重进行梯度计算\n",
        "#         param.requires_grad = True\n",
        "#     else:\n",
        "#         param.requires_grad = False\n",
        "\n",
        "# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)  # 仅传递需要梯度更新的参数给优化器\n",
        "optimizer = optim.Adam( model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        # batch_loaded_pairs = batch\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_calculator.loss(batch, batch_size)\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # optimizer.zero_grad()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item()}')\n",
        "\n",
        "    torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdlOnUpBKs68"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "\n",
        "def save_model_weights_to_hdf5(model, filepath):\n",
        "    with h5py.File(filepath, 'w') as f:\n",
        "        for name, param in model.named_parameters():\n",
        "            param_value = param.data.numpy()\n",
        "            f.create_dataset(name, data=param_value)\n",
        "\n",
        "def load_model_weights_from_hdf5(model, filepath):\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    with h5py.File(filepath, 'r') as f:\n",
        "        for name in f.keys():\n",
        "            param_value = torch.tensor(f[name][:])\n",
        "            model._parameters[name] = torch.nn.Parameter(param_value)\n",
        "\n",
        "\n",
        "save_model_weights_to_hdf5(model, '/content/drive/MyDrive/project_slam/model_weights.h5')\n",
        "\n",
        "# load_model_weights_from_hdf5(model, 'model_weights.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr0Hu_lV4pH2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
