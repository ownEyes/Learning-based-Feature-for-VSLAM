{"cells":[{"cell_type":"markdown","metadata":{"id":"Lg7OEpFGN7t9"},"source":["1.optimizer  2.print gradient 3. binarized layer deactivate no need, can be called  3.print weight"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5028,"status":"ok","timestamp":1713684651564,"user":{"displayName":"Jiaxuan Li","userId":"07141872132042446991"},"user_tz":-480},"id":"Fi1bf4mzWAt2"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","import os\n","import numpy as np\n","import cv2\n","import h5py\n","import random\n","import types\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","\n","from tensorboardX import SummaryWriter"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# %pip install tensorboardX\n","# %pip install tensorboard"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7fc3108fb990>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["seed = 1\n","random.seed(seed)\n","torch.manual_seed(seed)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def check_tensor(x, name):\n","    if torch.isnan(x).any() or torch.isinf(x).any():\n","        print(f\"NaN or Inf found in {name}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":344,"status":"ok","timestamp":1713684666063,"user":{"displayName":"Jiaxuan Li","userId":"07141872132042446991"},"user_tz":-480},"id":"TuSZdTlziXf_"},"outputs":[],"source":["def load_pairs_from_hdf5(hdf5_file_path, hdf5_folder):\n","    with h5py.File(hdf5_file_path, 'r') as hdf:\n","        loaded_pairs = []\n","\n","        # Function to convert a single relative path back to an absolute path\n","        def make_absolute(rel_path):\n","            # Decode if the path is a byte string\n","            if isinstance(rel_path, bytes):\n","                rel_path = rel_path.decode('utf-8')\n","            parts = rel_path.split('/')\n","            new_parts = []\n","            for part in parts:\n","                if part == 'sun3d_extracted' or part == '..':\n","                    continue\n","                new_parts.append(part)\n","            corrected_path = '/'.join(new_parts)\n","            absolute_path = os.path.join(hdf5_folder, corrected_path)\n","            return absolute_path\n","            # rel_path = rel_path.replace('../sun3d_extracted', '')\n","            # return os.path.join(hdf5_folder, rel_path)\n","\n","        # Function to process paths in pairs\n","        def process_paths(img_paths_array):\n","            # Ensure each path in the tuple is absolute\n","            return tuple(make_absolute(path) for path in img_paths_array)\n","\n","        # Load pairs\n","        pairs_group = hdf['pairs']\n","        for pair_name in pairs_group:\n","            pair_group = pairs_group[pair_name]\n","            # This will be a NumPy array\n","            img_paths_array = pair_group['img_paths'][()]\n","            # Process each path to be absolute\n","            img_paths = process_paths(img_paths_array)\n","            points1 = torch.tensor(pair_group['points1'][()])\n","            pos_points2 = torch.tensor(pair_group['pos_points2'][()])\n","            neg_points2 = torch.tensor(pair_group['neg_points2'][()])\n","            loaded_pairs.append({\n","                'img_paths': img_paths,\n","                'points1': points1,\n","                'pos_points2': pos_points2,\n","                'neg_points2': neg_points2\n","            })\n","\n","    return loaded_pairs"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"elapsed":360,"status":"error","timestamp":1713684668787,"user":{"displayName":"Jiaxuan Li","userId":"07141872132042446991"},"user_tz":-480},"id":"jOjFotKWiXgA","outputId":"c2aefd6b-839b-4026-a7eb-fac75f5c1403"},"outputs":[],"source":["# Get the current working directory\n","current_directory = os.getcwd()\n","output_path = os.path.join(current_directory, os.pardir,\n","                           os.pardir, 'datasets', 'sun3d_training')\n","# output_path = '/content/drive/MyDrive/project_slam/dataset/sun3d_training'\n","hdf5_file_path = os.path.join(output_path, 'pairs.hdf5')\n","# hdf5_file_path = '/content/drive/MyDrive/project_slam/dataset/sun3d_training/pairs.hdf5'\n","loaded_pairs = load_pairs_from_hdf5(hdf5_file_path, output_path)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":14195,"status":"ok","timestamp":1713684743589,"user":{"displayName":"Jiaxuan Li","userId":"07141872132042446991"},"user_tz":-480},"id":"nEIIo4qPPYqs"},"outputs":[],"source":["# Get the current working directory\n","# current_directory = os.getcwd()\n","# # output_path = os.path.join(current_directory, os.pardir, os.pardir,'sun3d_training')\n","# output_path = '/content/drive/MyDrive/project_slam/dataset/sun3d_training'\n","# # hdf5_file_path = os.path.join(output_path, 'pairs.hdf5')\n","# hdf5_file_path = '/content/drive/MyDrive/project_slam/dataset/sun3d_training/pairs.hdf5'\n","# loaded_pairs= load_pairs_from_hdf5(hdf5_file_path,output_path)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713691484923,"user":{"displayName":"Jiaxuan Li","userId":"07141872132042446991"},"user_tz":-480},"id":"yiUhO8xras1m"},"outputs":[],"source":["class BinarizedActivation(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input):\n","        ctx.save_for_backward(input)\n","        out = torch.sign(input)\n","        # out = torch.where(out == 0.0, torch.tensor(1.0), out)  # Change only 0 to 1\n","        # Ensures that all zero values become one, and all other values remain as they are.\n","        out = out + (out == 0.0).float()  # Adds 1 to only where out == 0\n","\n","        # Create a mask of elements that are neither 1.0 nor -1.0\n","        # mask = (out != 1.0) & (out != -1.0)\n","        # # Check if there are any such elements\n","        # if torch.any(mask):\n","        #     print(out[mask])  # Print only the elements that do not meet the condition\n","        return out\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        input, = ctx.saved_tensors\n","        grad_input = grad_output.clone()\n","        # print(\"Gradient from next layer: \", grad_output)\n","        # grad_input[(input < -1) | (input > 1)] = 0\n","        grad_input[(input.abs() > 1)] = 0\n","        # print(\"Modified gradient: \", grad_input)\n","        # print(\"BinarizedActivation backward called\")\n","        return grad_input\n","\n","\n","class GCNv2(nn.Module):\n","    def __init__(self):\n","        super(GCNv2, self).__init__()\n","        # self.elu = F.elu\n","        # self.elu = torch.nn.ELU(inplace=True)\n","        self.elu = torch.nn.ReLU(inplace=True)\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n","        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1)\n","        self.conv4_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=4, stride=2, padding=1)\n","        self.convF_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.convF_2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n","        self.convD_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","        self.convD_2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n","        # self.binarized_activation = BinarizedActivation()\n","        self.pixel_shuffle = nn.PixelShuffle(16)\n","        # Adjust scale factor as needed\n","        self.upsample = nn.Upsample(\n","            scale_factor=16, mode='bilinear', align_corners=True)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","                    nn.init.kaiming_normal_(\n","                        m.weight, mode='fan_in', nonlinearity='relu')\n","    # def reset_parameters(self):\n","    #     for m in self.modules():\n","    #         if isinstance(m, nn.Conv2d):\n","    #             # Glorot uniform initialization, also known as Xavier uniform initialization\n","    #             nn.init.xavier_uniform_(m.weight)\n","    #             if m.bias is not None:\n","    #                 nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, x):\n","        x.requires_grad_(True)\n","        x = self.elu(self.conv1(x))\n","        x = self.elu(self.conv2(x))\n","        x = self.elu(self.conv3_1(x))\n","        x = self.elu(self.conv3_2(x))\n","        x = self.elu(self.conv4_1(x))\n","        x = self.elu(self.conv4_2(x))\n","\n","        # Detector\n","        xD = self.elu(self.convD_1(x))\n","        det = self.convD_2(xD).sigmoid()\n","        det = self.pixel_shuffle(det)\n","\n","        # Descriptor\n","        xF = self.elu(self.convF_1(x))\n","        desc = self.convF_2(xF)\n","        desc = self.upsample(desc)\n","        dn = torch.norm(desc, p=2, dim=1)\n","        # desc = BinarizedActivation.apply(desc.div(torch.unsqueeze(dn, 1)))\n","        desc = desc.div(torch.unsqueeze(dn, 1))\n","\n","        # desc = F.interpolate(desc, size=det.shape[2:], mode='bilinear', align_corners=False)\n","        # desc = self.binarized_activation.apply(desc)\n","        # # Check if the norm across each channel is 1 (with a small tolerance for floating point arithmetic)\n","        # norms = torch.norm(desc, p=2, dim=1)\n","        # # Check norms and print those which are not approximately 1 (considering numerical stability)\n","        # not_normalized = (torch.abs(norms - 1) > 1e-6)\n","        # if not_normalized.any():\n","        #     print(\"Descriptors not normalized to 1:\", norms[not_normalized])\n","\n","        # print(desc.requires_grad)\n","        # print(\"desc shape\")\n","        # print(desc.shape)\n","        # print(\"det shape\")\n","        # print(det.shape)\n","\n","        return desc, det"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713691774033,"user":{"displayName":"Jiaxuan Li","userId":"07141872132042446991"},"user_tz":-480},"id":"kPEaQQhQ7wdg"},"outputs":[],"source":["class loss_calculator_3:\n","    def __init__(self, model):\n","        self.model = model.to(torch.device(\n","            'cuda' if torch.cuda.is_available() else 'cpu'))\n","        self.device = torch.device(\n","            'cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # def remove_padding(self, pts):\n","    #     mask = (pts != torch.tensor([float('inf'), float('inf')]))\n","    #     non_padding_vectors = pts[mask.all(dim=1)]\n","    #     return non_padding_vectors\n","\n","    def get_images_for_batch(self, img_paths):\n","        images = []\n","        # img_paths = batch_loaded_pairs['img_paths']\n","        for img_path in img_paths:\n","            img1, img2 = cv2.imread(img_path[0]), cv2.imread(img_path[1])\n","            gray_img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n","            gray_img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n","            img_tensor1 = torch.from_numpy(\n","                gray_img1).unsqueeze(0).float().to(self.device)\n","            img_tensor2 = torch.from_numpy(\n","                gray_img2).unsqueeze(0).float().to(self.device)\n","            images.append((img_tensor1, img_tensor2))\n","\n","        return images\n","\n","    # def normalized_pts(self, batch, height=480, width=640):\n","    #     normalized_pts = {}\n","    #     scale_w = float(width) / 2.\n","    #     scale_h = float(height) / 2.\n","    #     pts_cur = batch['points1'].clone()\n","    #     pts_tar_pos = batch['pos_points2'].clone()\n","    #     pts_tar_neg = batch['neg_points2'].clone()\n","\n","    #     normalized_pts['cur'] = torch.zeros_like(pts_cur)\n","    #     normalized_pts['tar_pos'] = torch.zeros_like(pts_tar_pos)\n","    #     normalized_pts['tar_neg'] = torch.zeros_like(pts_tar_neg)\n","    #     normalized_pts['cur'][:, :, 0] = pts_cur[:, :, 0] / scale_w - 1\n","    #     normalized_pts['cur'][:, :, 1] = pts_cur[:, :, 1] / scale_h - 1\n","    #     normalized_pts['tar_pos'][:, :, 0] = pts_tar_pos[:, :, 0] / scale_w - 1\n","    #     normalized_pts['tar_pos'][:, :, 1] = pts_tar_pos[:, :, 1] / scale_h - 1\n","    #     normalized_pts['tar_neg'][:, :, 0] = pts_tar_neg[:, :, 0] / scale_w - 1\n","    #     return normalized_pts\n","\n","    # def get_desc_pairs(self, batch, height=480, width=640):\n","    #     batchsize = len(batch.get('img_paths'))\n","    #     normal_pts = self.normalized_pts(batch, height, width)\n","    #     img_paths = batch.get('img_paths')\n","    #     gray_images = self.get_images_for_batch(img_paths)  # get img_paths\n","    #     desc_dict = {}\n","    #     for key, pts_group in normal_pts.items():\n","    #         desc = []\n","    #         for i in range(batchsize):\n","    #             inp_cur, inp_tar = gray_images[i]\n","    #             inp_cur = inp_cur.unsqueeze(0).float()\n","    #             inp_tar = inp_tar.unsqueeze(0).float()\n","    #             desc_cur, desc_tar = self.model(\n","    #                 inp_cur)[0], self.model(inp_tar)[0]\n","    #             desc_cur.requires_grad_(True)\n","    #             desc_tar.requires_grad_(True)\n","    #             desc_tensor = desc_cur if key == 'cur' else desc_tar\n","    #             if torch.isinf(pts_group[i]).any():\n","    #                 pts = self.remove_padding(pts_group[i]).view(\n","    #                     1, 1, -1, 2).float().to(self.device)\n","    #             else:\n","    #                 pts = pts_group[i].view(\n","    #                     1, 1, -1, 2).float().to(self.device)\n","    #             # sample_desc = F.grid_sample(desc_tensor, pts, align_corners=False, mode='nearest').squeeze()\n","    #             sample_desc = F.grid_sample(\n","    #                 desc_tensor, pts, align_corners=False, mode='nearest').squeeze()\n","    #             # Check if sample_desc contains float numbers\n","    #             # if not torch.all((sample_desc == 1.0) | (sample_desc == -1.0)):\n","    #             #   raise ValueError(\"sample_desc contains values other than 1.0 or -1.0.\")\n","    #             # mask = (sample_desc != 1.0) & (sample_desc != -1.0)\n","    #             # # Check if there are any such elements\n","    #             # if torch.any(mask):\n","    #             #     print(sample_desc[mask])  # Print only the elements that do not meet the condition\n","\n","    #             desc.append(sample_desc)\n","\n","    # Ensure to gather descriptors at specific keypoints\n","    # def gather_descriptors(self, desc_map, coords):\n","    #     if coords.nelement() == 0:\n","    #         print(\"no coords\")\n","    #         return torch.empty(0, desc_map.size(1), device=desc_map.device)\n","    #     # Convert (x, y) coordinates to indices\n","    #     indices = coords.long().to(desc_map.device)\n","    #     # Clamp coordinates to the size of the descriptor map\n","    #     indices[:, 0].clamp_(0, desc_map.size(3) - 1)  # x coordinates\n","    #     indices[:, 1].clamp_(0, desc_map.size(2) - 1)  # y coordinates\n","    #     # Gather descriptors\n","    #     sampled_desc = desc_map[0, :, indices[:, 1], indices[:, 0]].t()\n","    #     return sampled_desc\n","\n","    #         desc_dict[key] = desc\n","    #     return desc_dict\n","    def get_desc_pairs(self, batch, height=480, width=640):\n","        img_paths = batch.get('img_paths')\n","        batchsize = len(img_paths)\n","        gray_images = self.get_images_for_batch(img_paths)\n","        desc_dict = {'cur': [], 'tar_pos': [], 'tar_neg': []}\n","\n","        # Retrieve coordinate arrays from the batch\n","        coord_cur = batch.get('points1')\n","        coord_tar_pos = batch.get('pos_points2')\n","        coord_tar_neg = batch.get('neg_points2')\n","\n","        mask_cur = self.generate_batch_mask(\n","            coord_cur, height, width).to(self.device)\n","        mask_pos = self.generate_batch_mask(\n","            coord_tar_pos, height, width).to(self.device)\n","        mask_neg = self.generate_batch_mask(\n","            coord_tar_neg, height, width).to(self.device)\n","\n","        # print(\"mask shapes:\",\n","        #       mask_cur.shape, mask_pos.shape, mask_neg.shape)\n","\n","        for i in range(batchsize):\n","            inp_cur, inp_tar = gray_images[i]\n","            inp_cur = inp_cur.unsqueeze(0).float()\n","            inp_tar = inp_tar.unsqueeze(0).float()\n","\n","            # Get descriptor feature maps\n","            desc_cur, desc_tar = self.model(inp_cur)[0], self.model(inp_tar)[0]\n","            desc_cur.requires_grad_(True)\n","            desc_tar.requires_grad_(True)\n","\n","            # # Sample descriptors for current, positive, and negative keypoints\n","            # sampled_desc_cur = self.gather_descriptors(\n","            #     desc_cur, batch['points1'][i])\n","            # sampled_desc_pos = self.gather_descriptors(\n","            #     desc_tar, batch['pos_points2'][i])\n","            # sampled_desc_neg = self.gather_descriptors(\n","            #     desc_tar, batch['neg_points2'][i])\n","            # Generate indices from the mask\n","            nonzero_indices_cur = mask_cur[i].nonzero(as_tuple=False)\n","            nonzero_indices_pos = mask_pos[i].nonzero(as_tuple=False)\n","            nonzero_indices_neg = mask_neg[i].nonzero(as_tuple=False)\n","\n","            # Extract descriptors using the indices\n","            # We index with (y, x) because the first dimension is height (rows) and the second is width (columns)\n","            sampled_desc_cur = desc_cur[0, :,\n","                                        nonzero_indices_cur[:, 0], nonzero_indices_cur[:, 1]].t()\n","            sampled_desc_pos = desc_tar[0, :,\n","                                        nonzero_indices_pos[:, 0], nonzero_indices_pos[:, 1]].t()\n","            sampled_desc_neg = desc_tar[0, :,\n","                                        nonzero_indices_neg[:, 0], nonzero_indices_neg[:, 1]].t()\n","\n","            # Append to lists\n","            desc_dict['cur'].append(sampled_desc_cur)\n","            desc_dict['tar_pos'].append(sampled_desc_pos)\n","            desc_dict['tar_neg'].append(sampled_desc_neg)\n","\n","        desc_dict['cur'] = torch.cat(desc_dict['cur'])\n","        desc_dict['tar_pos'] = torch.cat(desc_dict['tar_pos'])\n","        desc_dict['tar_neg'] = torch.cat(desc_dict['tar_neg'])\n","\n","        # print(\"desc shapes\", desc_dict['cur'].shape,\n","        #       desc_dict['tar_pos'].shape, desc_dict['tar_neg'].shape)\n","\n","        return desc_dict\n","\n","    def batch_l_desc_loss(self, desc_batch, margin=1.0):\n","        ldesc = 0\n","        count_non_zero_losses = 0  # Counter for non-zero losses\n","        for cur_list, pos_list, neg_list in zip(desc_batch['cur'], desc_batch['tar_pos'], desc_batch['tar_neg']):\n","            # Calculate the norms of descriptor vectors\n","            # norms_cur = torch.norm(cur_list, p=2, dim=-1)\n","            # norms_pos = torch.norm(pos_list, p=2, dim=-1)\n","            # norms_neg = torch.norm(neg_list, p=2, dim=-1)\n","\n","            # # Check if any norms are greater than 1 and print them\n","            # if (torch.abs(norms_cur - 1) > 1e-6).any():\n","            #     print(\"Current descriptor norms greater than 1:\",\n","            #           norms_cur[norms_cur > 1])\n","            # if (torch.abs(norms_pos - 1) > 1e-6).any():\n","            #     print(\"Positive descriptor norms greater than 1:\",\n","            #           norms_pos[norms_pos > 1])\n","            # if (torch.abs(norms_neg - 1) > 1e-6).any():\n","            #     print(\"Negative descriptor norms greater than 1:\",\n","            #           norms_neg[norms_neg > 1])\n","\n","            # print(\"Shapes:\", cur_list.shape, pos_list.shape, neg_list.shape)\n","\n","            pairwise_dist_pos = torch.sum((cur_list - pos_list) ** 2, dim=-1)\n","            pairwise_dist_neg = torch.sum((cur_list - neg_list) ** 2, dim=-1)\n","            # Compute Euclidean distance without squaring\n","            # pairwise_dist_pos = torch.sqrt(\n","            #     torch.sum((cur_list - pos_list) ** 2, dim=-1))\n","            # check_tensor(pairwise_dist_pos, \"pos desc\")\n","            # pairwise_dist_neg = torch.sqrt(\n","            #     torch.sum((cur_list - neg_list) ** 2, dim=-1))\n","            # check_tensor(pairwise_dist_neg, \"neg desc\")\n","            # sample_loss = torch.sum(torch.max(torch.zeros_like(\n","            #     pairwise_dist_pos), pairwise_dist_pos - pairwise_dist_neg + margin))\n","            # Compute individual losses with margin\n","            individual_losses = torch.max(torch.zeros_like(\n","                pairwise_dist_pos), pairwise_dist_pos - pairwise_dist_neg + margin)\n","            sample_loss = torch.sum(individual_losses)\n","\n","            # Only add to the loss sum if it's non-zero\n","            if torch.any(individual_losses > 0):\n","                # Count how many are non-zero\n","                count_non_zero_losses += individual_losses.gt(0).sum().item()\n","                ldesc += sample_loss\n","\n","            # ldesc += sample_loss\n","        # Normalize the loss by the number of non-zero losses if there are any\n","        if count_non_zero_losses > 0:\n","            # print(\"total num of challenging pairs:\", count_non_zero_losses)\n","            ldesc = ldesc / count_non_zero_losses\n","        else:\n","            ldesc = torch.tensor(0.0).to(self.device)\n","\n","        return ldesc.to(self.device)\n","\n","    def get_det_pairs(self, batch):\n","        img_paths = batch.get('img_paths')\n","        batchsize = len(img_paths)\n","        gray_images = self.get_images_for_batch(img_paths)\n","        det_dict = {}\n","        det_cur_list = []\n","        det_tar_list = []\n","        for i in range(batchsize):\n","            inp_cur, inp_tar = gray_images[i]\n","            inp_cur = inp_cur.unsqueeze(0).float()\n","            inp_tar = inp_tar.unsqueeze(0).float()\n","            det_cur, det_tar = self.model(inp_cur)[1], self.model(inp_tar)[1]\n","            det_cur.requires_grad_(True)\n","            det_tar.requires_grad_(True)\n","            det_cur_list.append(det_cur.squeeze())\n","            det_tar_list.append(det_tar.squeeze())\n","        det_cur_list = torch.stack(det_cur_list, dim=0)\n","        det_tar_list = torch.stack(det_tar_list, dim=0)\n","        # print('det_list_shapes',det_cur_list.shape,det_tar_list.shape)\n","        det_dict = {'points1': det_cur_list, 'pos_points2': det_tar_list}\n","        return det_dict\n","\n","    def generate_batch_mask(self, coords, height=480, width=640):\n","\n","        mask = torch.zeros((coords.shape[0], height, width), dtype=torch.uint8)\n","        for batch_idx in range(coords.shape[0]):\n","            batch_coords = coords[batch_idx]\n","            for point_idx, (x, y) in enumerate(batch_coords):\n","                x = x.int()\n","                y = y.int()\n","                if 0 <= x < width and 0 <= y < height:\n","                    mask[batch_idx, y, x] = 1\n","\n","        return mask\n","\n","    def transform_target(self, batch, height=480, width=640):\n","        coord_cur = batch.get('points1')\n","        coord_tar = batch.get('pos_points2')\n","        # print(\"det shapes:\", coord_cur.shape, coord_tar.shape)\n","        cur_tensor = self.generate_batch_mask(coord_cur, height, width)\n","        tar_tensor = self.generate_batch_mask(coord_tar, height, width)\n","        return cur_tensor, tar_tensor\n","\n","    def l_det_loss(self, o_cur, c_cur, o_tar, c_tar, alpha1=1.0, alpha2=1.0):\n","        Lce_cur = self.binary_cross_entropy(o_cur, c_cur) / (480 * 640)\n","        Lce_tar = self.binary_cross_entropy(o_tar, c_tar) / (480 * 640)\n","        Ldet = alpha1 * Lce_cur + alpha2 * Lce_tar\n","        return Ldet\n","\n","    def binary_cross_entropy(self, o, c, epsilon=1e-8):\n","        c = c.to(o.dtype)\n","        positive = c.sum()\n","        total = c.numel()\n","        negative = total - positive\n","        # print(\"total pairs positive\")\n","        # print(positive)\n","        if positive.item() == 0:  # Avoid division by zero\n","            pos_weight = 1.0  # Default to 1 or handle it according to your context\n","        else:\n","            pos_weight = negative / positive\n","        weights = c * pos_weight + (1 - c) * 1.0\n","        # Ensuring that the target labels are within (epsilon, 1 - epsilon) to avoid log(0)\n","        c = torch.clamp(c, epsilon, 1 - epsilon)\n","        bce_loss = F.binary_cross_entropy_with_logits(\n","            o, c, reduction='none', weight=weights)\n","        total_loss = torch.sum(bce_loss)\n","        return total_loss\n","\n","    def batch_l_det_loss(self, batch, det_batch):\n","        ldet = 0\n","        # for i in range(len(batch_loaded_pairs)):\n","        cur_det = det_batch['points1']\n","        tar_det = det_batch['pos_points2']\n","        trans_cur = self.transform_target(batch, height=480, width=640)[\n","            0].to(self.device)\n","        trans_tar_pos = self.transform_target(batch, height=480, width=640)[\n","            1].to(self.device)\n","        # print('tar_shapes', trans_cur.shape, trans_tar_pos.shape)\n","        ldet += self.l_det_loss(cur_det, trans_cur, tar_det, trans_tar_pos)\n","        return ldet\n","\n","    def calculate_total_pairs(self, batch):\n","        valid_points_mask = ~torch.isinf(batch['pos_points2']).any(dim=2)\n","        # Sum over all true values in the mask to get the count of valid points\n","        total_pairs = valid_points_mask.sum().item()\n","        return total_pairs\n","\n","    def loss(self, batch_loaded_pairs, batch_size, height=480, width=640, margin=1.0):\n","        batchsize = len(batch_loaded_pairs.get('img_paths'))\n","\n","        det_batch = self.get_det_pairs(batch_loaded_pairs)\n","\n","        loss_det = self.batch_l_det_loss(batch_loaded_pairs, det_batch)\n","\n","        desc_batch = self.get_desc_pairs(batch_loaded_pairs, height, width)\n","        loss_desc = self.batch_l_desc_loss(desc_batch, margin)\n","        # Calculate the total number of pairs in the batch\n","        # total_pairs = self.calculate_total_pairs(batch_loaded_pairs)\n","        # if total_pairs == 0:\n","        #     total_pairs = 1  # To avoid division by zero\n","        # final_loss = (100.0*loss_desc / total_pairs) + (loss_det / batchsize)\n","        final_loss = 100.0*loss_desc + (loss_det / batchsize)\n","        return final_loss, loss_desc, loss_det/batchsize\n","        # print('batchsize')\n","        # print(batchsize)\n","        # print('total_pairs')\n","        # print(total_pairs)\n","        # return final_loss, loss_desc/total_pairs, loss_det/batchsize"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713691776296,"user":{"displayName":"Jiaxuan Li","userId":"07141872132042446991"},"user_tz":-480},"id":"Ng7ABLtJKYBT"},"outputs":[],"source":["model = GCNv2()\n","loss_calculator = loss_calculator_3(model)\n","\n","\n","def collate_fn(batch):\n","    max_len_points = max(len(sample['points1']) for sample in batch)\n","    for sample in batch:\n","        n = max_len_points - len(sample['points1'])\n","        inf_tensor = torch.tensor([[float('inf'), float('inf')]] * n)\n","        sample['points1'] = torch.cat((sample['points1'], inf_tensor), dim=0)\n","        sample['pos_points2'] = torch.cat(\n","            (sample['pos_points2'], inf_tensor), dim=0)\n","        sample['neg_points2'] = torch.cat(\n","            (sample['neg_points2'], inf_tensor), dim=0)\n","    img_path = [sample['img_paths'] for sample in batch]\n","    points1 = torch.stack([sample['points1'] for sample in batch], dim=0)\n","    pos_points2 = torch.stack([sample['pos_points2']\n","                              for sample in batch], dim=0)\n","    neg_points2 = torch.stack([sample['neg_points2']\n","                              for sample in batch], dim=0)\n","\n","    return {'img_paths': img_path, 'points1': points1, 'pos_points2': pos_points2, 'neg_points2': neg_points2}\n","\n","\n","# class Pairs(Dataset):\n","#     def __init__(self, loaded_pairs):\n","#         self.loaded_pairs = loaded_pairs\n","\n","#     def __len__(self):\n","#         return len(self.loaded_pairs)\n","\n","#     def __getitem__(self, idx):\n","#         pair = self.loaded_pairs[idx]\n","#         # path_tensor = torch.tensor(pair['img_paths'])\n","#         pts_tensor = pair['points1']\n","#         pos_tensor = pair['pos_points2']\n","#         neg_tensor = pair['neg_points2']\n","#         return {'img_paths': pair['img_paths'], 'points1': pts_tensor, 'pos_points2': pos_tensor, 'neg_points2': neg_tensor}\n","\n","\n","dataset = loaded_pairs\n","\n","batch_size = 4\n","dataloader = DataLoader(dataset, batch_size=batch_size,\n","                        shuffle=True, collate_fn=collate_fn)\n","# for batch in dataloader:\n","#   l_total = loss_calculator.loss(batch,batch_size)\n","#   print(l_total)\n","# print(count_ones)s"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":367,"status":"ok","timestamp":1713691778804,"user":{"displayName":"Jiaxuan Li","userId":"07141872132042446991"},"user_tz":-480},"id":"aozbYwj2NvXu"},"outputs":[],"source":["def adjust_learning_rate(optimizer: torch.optim.Optimizer, epoch: int) -> None:\n","    \"\"\"Halves the learning rate of the optimizer every 40 epochs.\n","\n","    Args:\n","    optimizer (torch.optim.Optimizer): The optimizer for which to adjust the learning rate.\n","    epoch (int): The current epoch number.\n","\n","    \"\"\"\n","    # Every 40 epochs, halve the learning rate\n","    if epoch % 40 == 0:\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = param_group['lr'] * 0.5"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["\n","# Define parameter groups with different learning rates\n","detector_parameters = [p for n, p in model.named_parameters() if 'convD' in n]\n","descriptor_parameters = [\n","    p for n, p in model.named_parameters() if 'convF' in n]\n","base_parameters = [p for n, p in model.named_parameters(\n",") if 'convD' not in n and 'convF' not in n]\n","\n","# Sometimes, you might want to ensure that every parameter is assigned only once\n","# Make sure no parameter is left behind, you might want to verify:\n","assert set(detector_parameters).isdisjoint(set(descriptor_parameters))\n","assert len(detector_parameters) + len(descriptor_parameters) + \\\n","    len(base_parameters) == len(list(model.parameters()))\n","\n","learning_rate = 0.0\n","base_learning_rate = 5e-8\n","det_learning_rate = 5e-5\n","desc_learning_rate = 1e-6\n","# Setting up the Adam optimizer with different learning rates for different groups\n","optimizer = optim.Adam([\n","    # Base parameters, use default or base learning rate\n","    {'params': base_parameters, 'lr': base_learning_rate},\n","    # Higher learning rate for detector\n","    {'params': detector_parameters, 'lr': det_learning_rate},\n","    # Different learning rate for descriptor\n","    {'params': descriptor_parameters, 'lr': desc_learning_rate}\n","], betas=(0.95, 0.999), eps=1e-8, weight_decay=5e-5)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)  # 仅传递需要梯度更新的参数给优化器\n","# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","# learning_rate = 0.00005\n","# optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(\n","#     0.95, 0.999), eps=1e-8, weight_decay=5e-5)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n","num_epochs = 10"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# class ReduceLRonPlateauPerStep:\n","#     def __init__(self, optimizer, factor=0.5, patience_steps=1000, min_lr=1e-15, verbose=False):\n","#         self.optimizer = optimizer\n","#         self.factor = factor\n","#         self.patience_steps = patience_steps\n","#         self.min_lr = min_lr\n","#         self.verbose = verbose\n","#         self.best_loss = float('inf')\n","#         self.num_bad_steps = 0\n","\n","#     def step(self, loss):\n","#         if loss < self.best_loss:\n","#             self.best_loss = loss\n","#             self.num_bad_steps = 0\n","#         else:\n","#             self.num_bad_steps += 1\n","\n","#         if self.num_bad_steps >= self.patience_steps:\n","#             for param_group in self.optimizer.param_groups:\n","#                 old_lr = param_group['lr']\n","#                 new_lr = max(old_lr * self.factor, self.min_lr)\n","#                 if old_lr - new_lr > 0:\n","#                     param_group['lr'] = new_lr\n","#                     if self.verbose:\n","#                         print(\n","#                             f\"Reducing learning rate from {old_lr} to {new_lr}.\")\n","#             self.num_bad_steps = 0\n","\n","\n","# scheduler = ReduceLRonPlateauPerStep(\n","#     optimizer, factor=0.95, patience_steps=500, verbose=True)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# class TrendBasedLRScheduler:\n","#     def __init__(self, optimizer, window_size=200, decrease_factor=0.5, min_lr=1e-16, max_lr=1e-2, verbose=False):\n","#         self.optimizer = optimizer\n","#         self.window_size = window_size\n","#         self.decrease_factor = decrease_factor\n","#         self.min_lr = min_lr\n","#         self.max_lr = max_lr\n","#         self.verbose = verbose\n","#         self.losses = []\n","#         self.last_slope = None\n","\n","#     def step(self, loss):\n","#         self.losses.append(loss)\n","#         if len(self.losses) > self.window_size:\n","#             self.losses.pop(0)\n","\n","#         if len(self.losses) == self.window_size:\n","#             # Calculate the slope of the linear fit to the losses\n","#             times = np.arange(self.window_size)\n","#             coeffs = np.polyfit(times, self.losses, 1)\n","#             current_slope = coeffs[0]  # Coefficient of x (time)\n","\n","#             if self.last_slope is not None and current_slope > 0 and self.last_slope < 0:\n","#                 # Loss trend changed from decreasing to increasing\n","#                 for param_group in self.optimizer.param_groups:\n","#                     new_lr = max(param_group['lr'] *\n","#                                  self.decrease_factor, self.min_lr)\n","#                     param_group['lr'] = new_lr\n","#                     if self.verbose:\n","#                         print(\n","#                             f\"Decreased learning rate to {new_lr} due to increasing loss trend.\")\n","\n","#             self.last_slope = current_slope  # Update the last known slope\n","\n","\n","# scheduler = TrendBasedLRScheduler(optimizer, verbose=True)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import datetime\n","\n","tensorboard_path = './log/'\n","current_time = datetime.datetime.now()\n","\n","# Format the current time as a string (e.g., YYYYMMDD-HHMMSS)\n","formatted_time = current_time.strftime('%Y%m%d-%H%M%S')\n","# Append the formatted time to the tensorboard path\n","tensorboard_path += 'time-{}'.format(formatted_time)\n","tensorboard_path += '-num_epochs-{}'.format(num_epochs)\n","tensorboard_path += '-batch_size-{}'.format(batch_size)\n","if learning_rate:\n","    tensorboard_path += '-learning_rate-{}'.format(learning_rate)\n","elif base_learning_rate:\n","    tensorboard_path += '-base_learning_rate-{}'.format(base_learning_rate)\n","    tensorboard_path += '-det_learning_rate-{}'.format(det_learning_rate)\n","    tensorboard_path += '-desc_learning_rate-{}'.format(desc_learning_rate)\n","tensorboard_path += '-random_seed-{}'.format(seed)\n","\n","\n","if not os.path.isdir(tensorboard_path):\n","    os.makedirs(tensorboard_path)\n","\n","writer = SummaryWriter(tensorboard_path)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def log_weights(model, writer, step):\n","    for name, param in model.named_parameters():\n","        writer.add_histogram(f\"weights/{name}\", param.data, step)\n","        if param.grad is not None:\n","            writer.add_histogram(f\"grads/{name}\", param.grad.data, step)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":32732,"status":"error","timestamp":1713691813310,"user":{"displayName":"Jiaxuan Li","userId":"07141872132042446991"},"user_tz":-480},"id":"vQIW2PjkqVwT","outputId":"a0f5574f-49e8-4e8c-8943-5171b0758235"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Step [100/18689], Loss: 99.946044921875\n","Epoch [1/10], Step [200/18689], Loss: 98.30538940429688\n","Epoch [1/10], Step [300/18689], Loss: 96.3205337524414\n","Epoch [1/10], Step [400/18689], Loss: 91.78264617919922\n","Epoch [1/10], Step [500/18689], Loss: 98.62808227539062\n","Epoch [1/10], Step [600/18689], Loss: 91.729736328125\n","Epoch [1/10], Step [700/18689], Loss: 91.33296203613281\n","Epoch [1/10], Step [800/18689], Loss: 93.52735137939453\n","Epoch [1/10], Step [900/18689], Loss: 90.42301177978516\n","Epoch [1/10], Step [1000/18689], Loss: 95.57615661621094\n","Epoch [1/10], Step [1100/18689], Loss: 85.71043395996094\n","Epoch [1/10], Step [1200/18689], Loss: 91.20927429199219\n","Epoch [1/10], Step [1300/18689], Loss: 99.64363861083984\n","Epoch [1/10], Step [1400/18689], Loss: 81.6548080444336\n","Epoch [1/10], Step [1500/18689], Loss: 88.99545288085938\n","Epoch [1/10], Step [1600/18689], Loss: 91.843505859375\n","Epoch [1/10], Step [1700/18689], Loss: 96.46175384521484\n","Epoch [1/10], Step [1800/18689], Loss: 90.03013610839844\n","Epoch [1/10], Step [1900/18689], Loss: 84.93970489501953\n","Epoch [1/10], Step [2000/18689], Loss: 87.1045150756836\n","Epoch [1/10], Step [2100/18689], Loss: 90.0460433959961\n","Epoch [1/10], Step [2200/18689], Loss: 90.9161605834961\n","Epoch [1/10], Step [2300/18689], Loss: 85.90111541748047\n","Epoch [1/10], Step [2400/18689], Loss: 92.4893798828125\n","Epoch [1/10], Step [2500/18689], Loss: 86.30064392089844\n","Epoch [1/10], Step [2600/18689], Loss: 89.28223419189453\n","Epoch [1/10], Step [2700/18689], Loss: 90.2884292602539\n","Epoch [1/10], Step [2800/18689], Loss: 89.31681823730469\n","Epoch [1/10], Step [2900/18689], Loss: 85.716796875\n","Epoch [1/10], Step [3000/18689], Loss: 95.66925048828125\n","Epoch [1/10], Step [3100/18689], Loss: 87.1868896484375\n","Epoch [1/10], Step [3200/18689], Loss: 89.81803131103516\n","Epoch [1/10], Step [3300/18689], Loss: 86.24927520751953\n","Epoch [1/10], Step [3400/18689], Loss: 92.73020935058594\n","Epoch [1/10], Step [3500/18689], Loss: 80.88697052001953\n","Epoch [1/10], Step [3600/18689], Loss: 87.42350006103516\n","Epoch [1/10], Step [3700/18689], Loss: 92.37551879882812\n","Epoch [1/10], Step [3800/18689], Loss: 89.82970428466797\n","Epoch [1/10], Step [3900/18689], Loss: 98.79619598388672\n","Epoch [1/10], Step [4000/18689], Loss: 90.06433868408203\n","Epoch [1/10], Step [4100/18689], Loss: 90.66252136230469\n","Epoch [1/10], Step [4200/18689], Loss: 97.25035095214844\n","Epoch [1/10], Step [4300/18689], Loss: 94.18049621582031\n","Epoch [1/10], Step [4400/18689], Loss: 81.37545776367188\n","Epoch [1/10], Step [4500/18689], Loss: 86.83162689208984\n","Epoch [1/10], Step [4600/18689], Loss: 87.18006134033203\n","Epoch [1/10], Step [4700/18689], Loss: 87.54695892333984\n","Epoch [1/10], Step [4800/18689], Loss: 95.73319244384766\n","Epoch [1/10], Step [4900/18689], Loss: 89.0335464477539\n","Epoch [1/10], Step [5000/18689], Loss: 93.73953247070312\n","Epoch [1/10], Step [5100/18689], Loss: 93.82988739013672\n","Epoch [1/10], Step [5200/18689], Loss: 81.71461486816406\n","Epoch [1/10], Step [5300/18689], Loss: 82.17902374267578\n","Epoch [1/10], Step [5400/18689], Loss: 92.58920288085938\n","Epoch [1/10], Step [5500/18689], Loss: 86.22827911376953\n","Epoch [1/10], Step [5600/18689], Loss: 87.2310562133789\n","Epoch [1/10], Step [5700/18689], Loss: 78.9020004272461\n","Epoch [1/10], Step [5800/18689], Loss: 94.49913024902344\n","Epoch [1/10], Step [5900/18689], Loss: 89.15418243408203\n","Epoch [1/10], Step [6000/18689], Loss: 84.88494873046875\n","Epoch [1/10], Step [6100/18689], Loss: 87.73052215576172\n","Epoch [1/10], Step [6200/18689], Loss: 81.75218200683594\n","Epoch [1/10], Step [6300/18689], Loss: 94.32825469970703\n","Epoch [1/10], Step [6400/18689], Loss: 83.79061126708984\n","Epoch [1/10], Step [6500/18689], Loss: 90.59867095947266\n","Epoch [1/10], Step [6600/18689], Loss: 95.8280029296875\n","Epoch [1/10], Step [6700/18689], Loss: 89.26466369628906\n","Epoch [1/10], Step [6800/18689], Loss: 94.09834289550781\n","Epoch [1/10], Step [6900/18689], Loss: 85.77355194091797\n","Epoch [1/10], Step [7000/18689], Loss: 83.92345428466797\n","Epoch [1/10], Step [7100/18689], Loss: 94.45301818847656\n","Epoch [1/10], Step [7200/18689], Loss: 90.0886001586914\n","Epoch [1/10], Step [7300/18689], Loss: 95.7926254272461\n","Epoch [1/10], Step [7400/18689], Loss: 82.26026916503906\n","Epoch [1/10], Step [7500/18689], Loss: 88.97296142578125\n","Epoch [1/10], Step [7600/18689], Loss: 95.42533111572266\n","Epoch [1/10], Step [7700/18689], Loss: 93.5235824584961\n","Epoch [1/10], Step [7800/18689], Loss: 86.20552062988281\n","Epoch [1/10], Step [7900/18689], Loss: 97.89276123046875\n","Epoch [1/10], Step [8000/18689], Loss: 93.1490707397461\n","Epoch [1/10], Step [8100/18689], Loss: 92.4013671875\n","Epoch [1/10], Step [8200/18689], Loss: 90.01789093017578\n","Epoch [1/10], Step [8300/18689], Loss: 93.92024230957031\n","Epoch [1/10], Step [8400/18689], Loss: 94.97306823730469\n","Epoch [1/10], Step [8500/18689], Loss: 83.20328521728516\n","Epoch [1/10], Step [8600/18689], Loss: 101.271484375\n","Epoch [1/10], Step [8700/18689], Loss: 76.80786895751953\n","Epoch [1/10], Step [8800/18689], Loss: 85.70732116699219\n","Epoch [1/10], Step [8900/18689], Loss: 89.3686294555664\n","Epoch [1/10], Step [9000/18689], Loss: 84.18984985351562\n","Epoch [1/10], Step [9100/18689], Loss: 91.40309143066406\n","Epoch [1/10], Step [9200/18689], Loss: 96.61372375488281\n","Epoch [1/10], Step [9300/18689], Loss: 82.57472229003906\n","Epoch [1/10], Step [9400/18689], Loss: 87.29652404785156\n","Epoch [1/10], Step [9500/18689], Loss: 90.60358428955078\n","Epoch [1/10], Step [9600/18689], Loss: 90.46536254882812\n","Epoch [1/10], Step [9700/18689], Loss: 94.90593719482422\n","Epoch [1/10], Step [9800/18689], Loss: 91.01461791992188\n","Epoch [1/10], Step [9900/18689], Loss: 93.94210815429688\n","Epoch [1/10], Step [10000/18689], Loss: 92.27062225341797\n","Epoch [1/10], Step [10100/18689], Loss: 85.46187591552734\n","Epoch [1/10], Step [10200/18689], Loss: 83.0239486694336\n","Epoch [1/10], Step [10300/18689], Loss: 86.97822570800781\n","Epoch [1/10], Step [10400/18689], Loss: 91.06670379638672\n","Epoch [1/10], Step [10500/18689], Loss: 86.49333953857422\n","Epoch [1/10], Step [10600/18689], Loss: 95.35079193115234\n","Epoch [1/10], Step [10700/18689], Loss: 89.25822448730469\n","Epoch [1/10], Step [10800/18689], Loss: 90.69229888916016\n","Epoch [1/10], Step [10900/18689], Loss: 94.9620132446289\n","Epoch [1/10], Step [11000/18689], Loss: 87.2398681640625\n","Epoch [1/10], Step [11100/18689], Loss: 94.67680358886719\n","Epoch [1/10], Step [11200/18689], Loss: 93.22193908691406\n","Epoch [1/10], Step [11300/18689], Loss: 90.75208282470703\n","Epoch [1/10], Step [11400/18689], Loss: 90.94637298583984\n","Epoch [1/10], Step [11500/18689], Loss: 82.07119750976562\n","Epoch [1/10], Step [11600/18689], Loss: 89.2281723022461\n","Epoch [1/10], Step [11700/18689], Loss: 79.09184265136719\n","Epoch [1/10], Step [11800/18689], Loss: 90.03495788574219\n","Epoch [1/10], Step [11900/18689], Loss: 97.2125473022461\n","Epoch [1/10], Step [12000/18689], Loss: 94.38838195800781\n","Epoch [1/10], Step [12100/18689], Loss: 82.36921691894531\n","Epoch [1/10], Step [12200/18689], Loss: 94.7407455444336\n","Epoch [1/10], Step [12300/18689], Loss: 93.48986053466797\n","Epoch [1/10], Step [12400/18689], Loss: 91.69796752929688\n","Epoch [1/10], Step [12500/18689], Loss: 96.44850158691406\n","Epoch [1/10], Step [12600/18689], Loss: 98.28921508789062\n","Epoch [1/10], Step [12700/18689], Loss: 89.41016387939453\n","Epoch [1/10], Step [12800/18689], Loss: 90.80043029785156\n","Epoch [1/10], Step [12900/18689], Loss: 87.12035369873047\n","Epoch [1/10], Step [13000/18689], Loss: 78.6825942993164\n","Epoch [1/10], Step [13100/18689], Loss: 90.52835083007812\n","Epoch [1/10], Step [13200/18689], Loss: 88.82942962646484\n","Epoch [1/10], Step [13300/18689], Loss: 94.22843170166016\n","Epoch [1/10], Step [13400/18689], Loss: 98.70655059814453\n","Epoch [1/10], Step [13500/18689], Loss: 93.90814208984375\n","Epoch [1/10], Step [13600/18689], Loss: 84.33929443359375\n","Epoch [1/10], Step [13700/18689], Loss: 91.64122009277344\n","Epoch [1/10], Step [13800/18689], Loss: 85.06292724609375\n","Epoch [1/10], Step [13900/18689], Loss: 91.58041381835938\n","Epoch [1/10], Step [14000/18689], Loss: 79.90106964111328\n","Epoch [1/10], Step [14100/18689], Loss: 88.02473449707031\n","Epoch [1/10], Step [14200/18689], Loss: 81.9184799194336\n","Epoch [1/10], Step [14300/18689], Loss: 95.31861877441406\n","Epoch [1/10], Step [14400/18689], Loss: 87.30401611328125\n","Epoch [1/10], Step [14500/18689], Loss: 92.41841888427734\n","Epoch [1/10], Step [14600/18689], Loss: 92.50215148925781\n","Epoch [1/10], Step [14700/18689], Loss: 86.02584838867188\n","Epoch [1/10], Step [14800/18689], Loss: 95.25151062011719\n","Epoch [1/10], Step [14900/18689], Loss: 81.11580657958984\n","Epoch [1/10], Step [15000/18689], Loss: 90.7264633178711\n","Epoch [1/10], Step [15100/18689], Loss: 95.60542297363281\n","Epoch [1/10], Step [15200/18689], Loss: 86.0235366821289\n","Epoch [1/10], Step [15300/18689], Loss: 85.53216552734375\n","Epoch [1/10], Step [15400/18689], Loss: 88.71481323242188\n","Epoch [1/10], Step [15500/18689], Loss: 93.88760375976562\n","Epoch [1/10], Step [15600/18689], Loss: 90.29438781738281\n","Epoch [1/10], Step [15700/18689], Loss: 83.33894348144531\n","Epoch [1/10], Step [15800/18689], Loss: 93.05266571044922\n","Epoch [1/10], Step [15900/18689], Loss: 89.08889770507812\n","Epoch [1/10], Step [16000/18689], Loss: 86.64571380615234\n","Epoch [1/10], Step [16100/18689], Loss: 74.8272705078125\n","Epoch [1/10], Step [16200/18689], Loss: 97.67957305908203\n","Epoch [1/10], Step [16300/18689], Loss: 94.5174789428711\n","Epoch [1/10], Step [16400/18689], Loss: 88.86640167236328\n","Epoch [1/10], Step [16500/18689], Loss: 92.19477844238281\n","Epoch [1/10], Step [16600/18689], Loss: 89.35661315917969\n","Epoch [1/10], Step [16700/18689], Loss: 88.7453384399414\n","Epoch [1/10], Step [16800/18689], Loss: 97.70975494384766\n","Epoch [1/10], Step [16900/18689], Loss: 90.32937622070312\n","Epoch [1/10], Step [17000/18689], Loss: 90.1406021118164\n","Epoch [1/10], Step [17100/18689], Loss: 92.31596374511719\n","Epoch [1/10], Step [17200/18689], Loss: 87.32991027832031\n","Epoch [1/10], Step [17300/18689], Loss: 92.15937805175781\n","Epoch [1/10], Step [17400/18689], Loss: 94.73558044433594\n","Epoch [1/10], Step [17500/18689], Loss: 96.65890502929688\n","Epoch [1/10], Step [17600/18689], Loss: 92.94419860839844\n","Epoch [1/10], Step [17700/18689], Loss: 89.17283630371094\n","Epoch [1/10], Step [17800/18689], Loss: 84.7385482788086\n","Epoch [1/10], Step [17900/18689], Loss: 88.29446411132812\n","Epoch [1/10], Step [18000/18689], Loss: 94.64220428466797\n","Epoch [1/10], Step [18100/18689], Loss: 91.24678802490234\n","Epoch [1/10], Step [18200/18689], Loss: 97.76229095458984\n","Epoch [1/10], Step [18300/18689], Loss: 95.10266876220703\n","Epoch [1/10], Step [18400/18689], Loss: 94.87256622314453\n","Epoch [1/10], Step [18500/18689], Loss: 89.07051086425781\n","Epoch [1/10], Step [18600/18689], Loss: 91.94770812988281\n","Epoch [2/10], Step [100/18689], Loss: 85.3261489868164\n","Epoch [2/10], Step [200/18689], Loss: 91.32840728759766\n","Epoch [2/10], Step [300/18689], Loss: 89.69676208496094\n","Epoch [2/10], Step [400/18689], Loss: 89.61194610595703\n","Epoch [2/10], Step [500/18689], Loss: 90.45331573486328\n","Epoch [2/10], Step [600/18689], Loss: 86.17819213867188\n","Epoch [2/10], Step [700/18689], Loss: 91.12910461425781\n","Epoch [2/10], Step [800/18689], Loss: 94.56111907958984\n","Epoch [2/10], Step [900/18689], Loss: 86.86534118652344\n","Epoch [2/10], Step [1000/18689], Loss: 93.72608947753906\n","Epoch [2/10], Step [1100/18689], Loss: 87.22029113769531\n","Epoch [2/10], Step [1200/18689], Loss: 95.74934387207031\n","Epoch [2/10], Step [1300/18689], Loss: 89.89518737792969\n","Epoch [2/10], Step [1400/18689], Loss: 80.58541107177734\n","Epoch [2/10], Step [1500/18689], Loss: 93.33322143554688\n","Epoch [2/10], Step [1600/18689], Loss: 91.85470581054688\n","Epoch [2/10], Step [1700/18689], Loss: 85.51173400878906\n","Epoch [2/10], Step [1800/18689], Loss: 91.73077392578125\n","Epoch [2/10], Step [1900/18689], Loss: 93.18186950683594\n","Epoch [2/10], Step [2000/18689], Loss: 96.33357238769531\n","Epoch [2/10], Step [2100/18689], Loss: 83.55891418457031\n","Epoch [2/10], Step [2200/18689], Loss: 85.3260269165039\n","Epoch [2/10], Step [2300/18689], Loss: 92.63795471191406\n","Epoch [2/10], Step [2400/18689], Loss: 90.76699829101562\n","Epoch [2/10], Step [2500/18689], Loss: 91.20442962646484\n","Epoch [2/10], Step [2600/18689], Loss: 80.98600006103516\n","Epoch [2/10], Step [2700/18689], Loss: 90.58435821533203\n","Epoch [2/10], Step [2800/18689], Loss: 85.0670166015625\n","Epoch [2/10], Step [2900/18689], Loss: 96.73194122314453\n","Epoch [2/10], Step [3000/18689], Loss: 91.06451416015625\n","Epoch [2/10], Step [3100/18689], Loss: 93.55354309082031\n","Epoch [2/10], Step [3200/18689], Loss: 86.79598236083984\n","Epoch [2/10], Step [3300/18689], Loss: 97.7034912109375\n","Epoch [2/10], Step [3400/18689], Loss: 85.52381134033203\n","Epoch [2/10], Step [3500/18689], Loss: 94.7291030883789\n","Epoch [2/10], Step [3600/18689], Loss: 85.89946746826172\n","Epoch [2/10], Step [3700/18689], Loss: 84.94847869873047\n","Epoch [2/10], Step [3800/18689], Loss: 92.11419677734375\n","Epoch [2/10], Step [3900/18689], Loss: 81.89897918701172\n","Epoch [2/10], Step [4000/18689], Loss: 90.92811584472656\n","Epoch [2/10], Step [4100/18689], Loss: 88.26824951171875\n","Epoch [2/10], Step [4200/18689], Loss: 89.26956939697266\n","Epoch [2/10], Step [4300/18689], Loss: 91.5260009765625\n","Epoch [2/10], Step [4400/18689], Loss: 87.1178207397461\n","Epoch [2/10], Step [4500/18689], Loss: 92.77378845214844\n","Epoch [2/10], Step [4600/18689], Loss: 99.35606384277344\n","Epoch [2/10], Step [4700/18689], Loss: 86.64170837402344\n","Epoch [2/10], Step [4800/18689], Loss: 94.85084533691406\n","Epoch [2/10], Step [4900/18689], Loss: 88.22754669189453\n","Epoch [2/10], Step [5000/18689], Loss: 88.4495849609375\n","Epoch [2/10], Step [5100/18689], Loss: 94.98731231689453\n","Epoch [2/10], Step [5200/18689], Loss: 81.49244689941406\n","Epoch [2/10], Step [5300/18689], Loss: 97.52470397949219\n","Epoch [2/10], Step [5400/18689], Loss: 90.56684875488281\n","Epoch [2/10], Step [5500/18689], Loss: 95.90354919433594\n","Epoch [2/10], Step [5600/18689], Loss: 86.67643737792969\n","Epoch [2/10], Step [5700/18689], Loss: 88.26980590820312\n","Epoch [2/10], Step [5800/18689], Loss: 84.96672058105469\n","Epoch [2/10], Step [5900/18689], Loss: 87.32635498046875\n","Epoch [2/10], Step [6000/18689], Loss: 88.5925064086914\n","Epoch [2/10], Step [6100/18689], Loss: 94.20868682861328\n","Epoch [2/10], Step [6200/18689], Loss: 80.8458251953125\n","Epoch [2/10], Step [6300/18689], Loss: 89.1595687866211\n","Epoch [2/10], Step [6400/18689], Loss: 89.1910629272461\n","Epoch [2/10], Step [6500/18689], Loss: 82.50006866455078\n","Epoch [2/10], Step [6600/18689], Loss: 85.11076354980469\n","Epoch [2/10], Step [6700/18689], Loss: 93.07437133789062\n","Epoch [2/10], Step [6800/18689], Loss: 85.01380157470703\n","Epoch [2/10], Step [6900/18689], Loss: 92.79605102539062\n","Epoch [2/10], Step [7000/18689], Loss: 91.19001007080078\n","Epoch [2/10], Step [7100/18689], Loss: 92.54045867919922\n","Epoch [2/10], Step [7200/18689], Loss: 96.88275146484375\n","Epoch [2/10], Step [7300/18689], Loss: 94.43657684326172\n","Epoch [2/10], Step [7400/18689], Loss: 91.05540466308594\n","Epoch [2/10], Step [7500/18689], Loss: 89.02377319335938\n","Epoch [2/10], Step [7600/18689], Loss: 80.61724853515625\n","Epoch [2/10], Step [7700/18689], Loss: 82.6167984008789\n","Epoch [2/10], Step [7800/18689], Loss: 95.34061431884766\n","Epoch [2/10], Step [7900/18689], Loss: 89.28889465332031\n","Epoch [2/10], Step [8000/18689], Loss: 86.80146026611328\n","Epoch [2/10], Step [8100/18689], Loss: 87.11903381347656\n","Epoch [2/10], Step [8200/18689], Loss: 86.71238708496094\n","Epoch [2/10], Step [8300/18689], Loss: 89.5691909790039\n","Epoch [2/10], Step [8400/18689], Loss: 77.06452178955078\n","Epoch [2/10], Step [8500/18689], Loss: 89.2711410522461\n","Epoch [2/10], Step [8600/18689], Loss: 88.93026733398438\n","Epoch [2/10], Step [8700/18689], Loss: 96.70938110351562\n","Epoch [2/10], Step [8800/18689], Loss: 88.6100082397461\n","Epoch [2/10], Step [8900/18689], Loss: 80.554931640625\n","Epoch [2/10], Step [9000/18689], Loss: 85.72769165039062\n","Epoch [2/10], Step [9100/18689], Loss: 91.84196472167969\n","Epoch [2/10], Step [9200/18689], Loss: 88.31915283203125\n","Epoch [2/10], Step [9300/18689], Loss: 91.77525329589844\n","Epoch [2/10], Step [9400/18689], Loss: 90.58985900878906\n","Epoch [2/10], Step [9500/18689], Loss: 90.06901550292969\n","Epoch [2/10], Step [9600/18689], Loss: 88.60883331298828\n","Epoch [2/10], Step [9700/18689], Loss: 74.00247192382812\n","Epoch [2/10], Step [9800/18689], Loss: 95.25237274169922\n","Epoch [2/10], Step [9900/18689], Loss: 87.65251159667969\n","Epoch [2/10], Step [10000/18689], Loss: 90.32476806640625\n","Epoch [2/10], Step [10100/18689], Loss: 99.75408172607422\n","Epoch [2/10], Step [10200/18689], Loss: 97.26736450195312\n","Epoch [2/10], Step [10300/18689], Loss: 93.48130798339844\n","Epoch [2/10], Step [10400/18689], Loss: 90.62794494628906\n","Epoch [2/10], Step [10500/18689], Loss: 86.38705444335938\n","Epoch [2/10], Step [10600/18689], Loss: 93.35440063476562\n","Epoch [2/10], Step [10700/18689], Loss: 88.36268615722656\n","Epoch [2/10], Step [10800/18689], Loss: 92.4043197631836\n","Epoch [2/10], Step [10900/18689], Loss: 80.04609680175781\n","Epoch [2/10], Step [11000/18689], Loss: 87.7598876953125\n","Epoch [2/10], Step [11100/18689], Loss: 87.58473205566406\n","Epoch [2/10], Step [11200/18689], Loss: 86.79749298095703\n","Epoch [2/10], Step [11300/18689], Loss: 90.83830261230469\n","Epoch [2/10], Step [11400/18689], Loss: 93.63949584960938\n","Epoch [2/10], Step [11500/18689], Loss: 85.24008178710938\n","Epoch [2/10], Step [11600/18689], Loss: 86.45269012451172\n","Epoch [2/10], Step [11700/18689], Loss: 89.65828704833984\n","Epoch [2/10], Step [11800/18689], Loss: 94.62926483154297\n","Epoch [2/10], Step [11900/18689], Loss: 87.61380767822266\n","Epoch [2/10], Step [12000/18689], Loss: 81.68074035644531\n","Epoch [2/10], Step [12100/18689], Loss: 95.71733093261719\n","Epoch [2/10], Step [12200/18689], Loss: 96.74332427978516\n","Epoch [2/10], Step [12300/18689], Loss: 101.24844360351562\n","Epoch [2/10], Step [12400/18689], Loss: 97.54524230957031\n","Epoch [2/10], Step [12500/18689], Loss: 89.80175018310547\n","Epoch [2/10], Step [12600/18689], Loss: 89.72142791748047\n","Epoch [2/10], Step [12700/18689], Loss: 88.36491394042969\n","Epoch [2/10], Step [12800/18689], Loss: 95.8432388305664\n","Epoch [2/10], Step [12900/18689], Loss: 95.2157974243164\n","Epoch [2/10], Step [13000/18689], Loss: 87.00071716308594\n","Epoch [2/10], Step [13100/18689], Loss: 91.85992431640625\n","Epoch [2/10], Step [13200/18689], Loss: 94.62140655517578\n","Epoch [2/10], Step [13300/18689], Loss: 86.45463562011719\n","Epoch [2/10], Step [13400/18689], Loss: 93.36434936523438\n","Epoch [2/10], Step [13500/18689], Loss: 84.2048110961914\n","Epoch [2/10], Step [13600/18689], Loss: 78.2513427734375\n","Epoch [2/10], Step [13700/18689], Loss: 78.27873992919922\n","Epoch [2/10], Step [13800/18689], Loss: 89.69219970703125\n","Epoch [2/10], Step [13900/18689], Loss: 84.68984985351562\n","Epoch [2/10], Step [14000/18689], Loss: 92.96797180175781\n","Epoch [2/10], Step [14100/18689], Loss: 83.62728118896484\n","Epoch [2/10], Step [14200/18689], Loss: 95.76810455322266\n","Epoch [2/10], Step [14300/18689], Loss: 84.40747833251953\n","Epoch [2/10], Step [14400/18689], Loss: 92.69367218017578\n","Epoch [2/10], Step [14500/18689], Loss: 97.73369598388672\n","Epoch [2/10], Step [14600/18689], Loss: 79.91854095458984\n","Epoch [2/10], Step [14700/18689], Loss: 80.2185287475586\n","Epoch [2/10], Step [14800/18689], Loss: 87.21802520751953\n","Epoch [2/10], Step [14900/18689], Loss: 90.23950958251953\n","Epoch [2/10], Step [15000/18689], Loss: 83.8954086303711\n","Epoch [2/10], Step [15100/18689], Loss: 94.19092559814453\n","Epoch [2/10], Step [15200/18689], Loss: 90.74464416503906\n","Epoch [2/10], Step [15300/18689], Loss: 92.0645751953125\n","Epoch [2/10], Step [15400/18689], Loss: 91.34444427490234\n","Epoch [2/10], Step [15500/18689], Loss: 82.20046997070312\n","Epoch [2/10], Step [15600/18689], Loss: 93.49122619628906\n","Epoch [2/10], Step [15700/18689], Loss: 87.7581558227539\n","Epoch [2/10], Step [15800/18689], Loss: 86.46460723876953\n","Epoch [2/10], Step [15900/18689], Loss: 92.99082946777344\n","Epoch [2/10], Step [16000/18689], Loss: 87.6649169921875\n","Epoch [2/10], Step [16100/18689], Loss: 91.44921112060547\n","Epoch [2/10], Step [16200/18689], Loss: 88.44947814941406\n","Epoch [2/10], Step [16300/18689], Loss: 85.10558319091797\n","Epoch [2/10], Step [16400/18689], Loss: 88.8967514038086\n","Epoch [2/10], Step [16500/18689], Loss: 90.9577407836914\n","Epoch [2/10], Step [16600/18689], Loss: 88.41356658935547\n","Epoch [2/10], Step [16700/18689], Loss: 89.71197509765625\n","Epoch [2/10], Step [16800/18689], Loss: 79.71812438964844\n","Epoch [2/10], Step [16900/18689], Loss: 86.69208526611328\n","Epoch [2/10], Step [17000/18689], Loss: 87.29766082763672\n","Epoch [2/10], Step [17100/18689], Loss: 79.54895782470703\n","Epoch [2/10], Step [17200/18689], Loss: 94.73139190673828\n","Epoch [2/10], Step [17300/18689], Loss: 93.95803833007812\n","Epoch [2/10], Step [17400/18689], Loss: 91.41035461425781\n","Epoch [2/10], Step [17500/18689], Loss: 78.45773315429688\n","Epoch [2/10], Step [17600/18689], Loss: 92.84800720214844\n","Epoch [2/10], Step [17700/18689], Loss: 94.56214904785156\n","Epoch [2/10], Step [17800/18689], Loss: 79.23466491699219\n","Epoch [2/10], Step [17900/18689], Loss: 85.62706756591797\n","Epoch [2/10], Step [18000/18689], Loss: 93.79102325439453\n","Epoch [2/10], Step [18100/18689], Loss: 93.69634246826172\n","Epoch [2/10], Step [18200/18689], Loss: 81.12727355957031\n","Epoch [2/10], Step [18300/18689], Loss: 86.8283462524414\n","Epoch [2/10], Step [18400/18689], Loss: 89.61604309082031\n","Epoch [2/10], Step [18500/18689], Loss: 87.68653869628906\n","Epoch [2/10], Step [18600/18689], Loss: 78.59700775146484\n","Epoch [3/10], Step [100/18689], Loss: 89.3175048828125\n","Epoch [3/10], Step [200/18689], Loss: 93.8941879272461\n","Epoch [3/10], Step [300/18689], Loss: 97.00984954833984\n","Epoch [3/10], Step [400/18689], Loss: 92.45459747314453\n","Epoch [3/10], Step [500/18689], Loss: 99.05602264404297\n","Epoch [3/10], Step [600/18689], Loss: 79.97171783447266\n","Epoch [3/10], Step [700/18689], Loss: 95.3298110961914\n","Epoch [3/10], Step [800/18689], Loss: 90.33845520019531\n","Epoch [3/10], Step [900/18689], Loss: 89.2757797241211\n","Epoch [3/10], Step [1000/18689], Loss: 88.2945556640625\n","Epoch [3/10], Step [1100/18689], Loss: 95.24512481689453\n","Epoch [3/10], Step [1200/18689], Loss: 97.68077850341797\n","Epoch [3/10], Step [1300/18689], Loss: 93.45442199707031\n","Epoch [3/10], Step [1400/18689], Loss: 84.00392150878906\n","Epoch [3/10], Step [1500/18689], Loss: 87.69795989990234\n","Epoch [3/10], Step [1600/18689], Loss: 85.77530670166016\n","Epoch [3/10], Step [1700/18689], Loss: 85.33189392089844\n","Epoch [3/10], Step [1800/18689], Loss: 89.67301940917969\n","Epoch [3/10], Step [1900/18689], Loss: 88.3530502319336\n","Epoch [3/10], Step [2000/18689], Loss: 86.23490905761719\n","Epoch [3/10], Step [2100/18689], Loss: 85.33473205566406\n","Epoch [3/10], Step [2200/18689], Loss: 98.51766204833984\n","Epoch [3/10], Step [2300/18689], Loss: 89.39971923828125\n","Epoch [3/10], Step [2400/18689], Loss: 89.7421646118164\n","Epoch [3/10], Step [2500/18689], Loss: 94.85255432128906\n","Epoch [3/10], Step [2600/18689], Loss: 85.29802703857422\n","Epoch [3/10], Step [2700/18689], Loss: 89.10938262939453\n","Epoch [3/10], Step [2800/18689], Loss: 89.46707916259766\n","Epoch [3/10], Step [2900/18689], Loss: 92.75419616699219\n","Epoch [3/10], Step [3000/18689], Loss: 84.9856948852539\n","Epoch [3/10], Step [3100/18689], Loss: 93.27799987792969\n","Epoch [3/10], Step [3200/18689], Loss: 94.9521255493164\n","Epoch [3/10], Step [3300/18689], Loss: 89.36640167236328\n","Epoch [3/10], Step [3400/18689], Loss: 91.0068130493164\n","Epoch [3/10], Step [3500/18689], Loss: 94.50143432617188\n","Epoch [3/10], Step [3600/18689], Loss: 86.90383911132812\n","Epoch [3/10], Step [3700/18689], Loss: 90.08873748779297\n","Epoch [3/10], Step [3800/18689], Loss: 93.93072509765625\n","Epoch [3/10], Step [3900/18689], Loss: 98.88943481445312\n","Epoch [3/10], Step [4000/18689], Loss: 98.3248062133789\n","Epoch [3/10], Step [4100/18689], Loss: 89.69947814941406\n","Epoch [3/10], Step [4200/18689], Loss: 93.376953125\n","Epoch [3/10], Step [4300/18689], Loss: 97.35588836669922\n","Epoch [3/10], Step [4400/18689], Loss: 95.21300506591797\n","Epoch [3/10], Step [4500/18689], Loss: 94.7565689086914\n","Epoch [3/10], Step [4600/18689], Loss: 97.85801696777344\n","Epoch [3/10], Step [4700/18689], Loss: 85.90296936035156\n","Epoch [3/10], Step [4800/18689], Loss: 91.36450958251953\n","Epoch [3/10], Step [4900/18689], Loss: 95.36619567871094\n","Epoch [3/10], Step [5000/18689], Loss: 86.39448547363281\n","Epoch [3/10], Step [5100/18689], Loss: 85.33463287353516\n","Epoch [3/10], Step [5200/18689], Loss: 86.65968322753906\n","Epoch [3/10], Step [5300/18689], Loss: 87.9166488647461\n","Epoch [3/10], Step [5400/18689], Loss: 94.12469482421875\n","Epoch [3/10], Step [5500/18689], Loss: 87.7631607055664\n","Epoch [3/10], Step [5600/18689], Loss: 94.79785919189453\n","Epoch [3/10], Step [5700/18689], Loss: 89.62713623046875\n","Epoch [3/10], Step [5800/18689], Loss: 91.67986297607422\n","Epoch [3/10], Step [5900/18689], Loss: 95.8051986694336\n","Epoch [3/10], Step [6000/18689], Loss: 91.20018768310547\n","Epoch [3/10], Step [6100/18689], Loss: 91.2474136352539\n","Epoch [3/10], Step [6200/18689], Loss: 83.91321563720703\n","Epoch [3/10], Step [6300/18689], Loss: 97.78933715820312\n","Epoch [3/10], Step [6400/18689], Loss: 85.12983703613281\n","Epoch [3/10], Step [6500/18689], Loss: 91.31306457519531\n","Epoch [3/10], Step [6600/18689], Loss: 86.40480041503906\n","Epoch [3/10], Step [6700/18689], Loss: 104.15118408203125\n","Epoch [3/10], Step [6800/18689], Loss: 87.2303695678711\n","Epoch [3/10], Step [6900/18689], Loss: 87.58480072021484\n","Epoch [3/10], Step [7000/18689], Loss: 88.88543701171875\n","Epoch [3/10], Step [7100/18689], Loss: 94.82502746582031\n","Epoch [3/10], Step [7200/18689], Loss: 91.07901000976562\n","Epoch [3/10], Step [7300/18689], Loss: 86.12994384765625\n","Epoch [3/10], Step [7400/18689], Loss: 92.28511047363281\n","Epoch [3/10], Step [7500/18689], Loss: 75.12647247314453\n","Epoch [3/10], Step [7600/18689], Loss: 85.478515625\n","Epoch [3/10], Step [7700/18689], Loss: 94.6508560180664\n","Epoch [3/10], Step [7800/18689], Loss: 92.6923828125\n","Epoch [3/10], Step [7900/18689], Loss: 96.8692855834961\n","Epoch [3/10], Step [8000/18689], Loss: 99.93561553955078\n","Epoch [3/10], Step [8100/18689], Loss: 88.39965057373047\n","Epoch [3/10], Step [8200/18689], Loss: 92.2593002319336\n","Epoch [3/10], Step [8300/18689], Loss: 86.89498901367188\n","Epoch [3/10], Step [8400/18689], Loss: 89.77395629882812\n","Epoch [3/10], Step [8500/18689], Loss: 88.88473510742188\n","Epoch [3/10], Step [8600/18689], Loss: 89.1315689086914\n","Epoch [3/10], Step [8700/18689], Loss: 93.11605072021484\n","Epoch [3/10], Step [8800/18689], Loss: 91.96814727783203\n","Epoch [3/10], Step [8900/18689], Loss: 83.94217681884766\n","Epoch [3/10], Step [9000/18689], Loss: 96.08296203613281\n","Epoch [3/10], Step [9100/18689], Loss: 90.17131805419922\n","Epoch [3/10], Step [9200/18689], Loss: 89.83457946777344\n","Epoch [3/10], Step [9300/18689], Loss: 95.56009674072266\n","Epoch [3/10], Step [9400/18689], Loss: 88.62024688720703\n","Epoch [3/10], Step [9500/18689], Loss: 89.96673583984375\n","Epoch [3/10], Step [9600/18689], Loss: 93.01644897460938\n","Epoch [3/10], Step [9700/18689], Loss: 92.83905792236328\n","Epoch [3/10], Step [9800/18689], Loss: 91.2079086303711\n","Epoch [3/10], Step [9900/18689], Loss: 90.88786315917969\n","Epoch [3/10], Step [10000/18689], Loss: 92.34066772460938\n","Epoch [3/10], Step [10100/18689], Loss: 87.72354888916016\n","Epoch [3/10], Step [10200/18689], Loss: 80.89453887939453\n","Epoch [3/10], Step [10300/18689], Loss: 85.98152923583984\n","Epoch [3/10], Step [10400/18689], Loss: 89.76384735107422\n","Epoch [3/10], Step [10500/18689], Loss: 88.81071472167969\n","Epoch [3/10], Step [10600/18689], Loss: 94.07011413574219\n","Epoch [3/10], Step [10700/18689], Loss: 88.4801254272461\n","Epoch [3/10], Step [10800/18689], Loss: 86.45342254638672\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during loss_desc.backward(): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/ros2_dl/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/ros2_dl/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# for name, param in model.named_parameters():\n","#     if 'weight' in name:  # 只对权重进行梯度计算\n","#         param.requires_grad = True\n","#     else:\n","#         param.requires_grad = False\n","\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for i, batch in enumerate(dataloader):\n","        # batch_loaded_pairs = batch\n","        optimizer.zero_grad()\n","        # desc= loss_calculator.get_desc_pairs(batch, height=480, width=640)\n","        loss, loss_desc, loss_det = loss_calculator.loss(batch, batch_size)\n","        # print(loss)\n","        # loss.backward()\n","\n","        try:\n","            loss.backward()\n","        except RuntimeError as e:\n","            print(f\"Error during loss_desc.backward(): {e}\")\n","\n","        # for name, param in model.named_parameters():\n","        #   if param.requires_grad:\n","        #       try:\n","        #           print(f\"{name} gradient: {param.grad}\")\n","        #       except AttributeError:\n","        #           print(f\"{name} gradient: None\")\n","\n","        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        # nan_detected = False\n","        # for name, param in model.named_parameters():\n","        #     if param.grad is not None and torch.isnan(param.grad).any():\n","        #         print(f\"NaN gradients detected in {name}\")\n","        #         nan_detected = True\n","\n","        # norm = torch.sqrt(sum(p.grad.data.norm()**2 for p in model.parameters() if p.grad is not None))\n","        # print(f\"Current gradient norm: {norm.item()}\")\n","\n","        optimizer.step()\n","        if (epoch * len(dataloader) + i) % 700 == 0:\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = max(param_group['lr'] * 0.3, 1e-50)\n","        # if epoch * len(dataloader) + i > 200:\n","        #     scheduler.step(loss_desc.item())\n","        # optimizer.zero_grad()\n","        writer.add_scalar(\n","            'Training/Loss', loss.item(), epoch * len(dataloader) + i)\n","        writer.add_scalar(\"Loss/Descriptor\", loss_desc.item(),\n","                          epoch * len(dataloader) + i)\n","        writer.add_scalar(\"Loss/Detector\", loss_det.item(),\n","                          epoch * len(dataloader) + i)\n","        # writer.add_scalar(\n","        #     'Training/Learning Rate', optimizer.param_groups[0][\"lr\"], epoch * len(dataloader) + i)\n","        for index, param_group in enumerate(optimizer.param_groups):\n","            writer.add_scalar(\n","                f'Training/Learning Rate/Group_{index}',\n","                param_group[\"lr\"],\n","                epoch * len(dataloader) + i)\n","\n","        if (i + 1) % 100 == 0:\n","            print(\n","                f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item()}')\n","            log_weights(model, writer, epoch * len(dataloader) + i)\n","\n","    torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n","    # Save the entire model\n","    # torch.save(model, f'model_epoch_{epoch+1}.pt')\n","writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.eval()\n","# Adjust input dimensions as necessary\n","example_input = torch.rand(1, 1, 480, 640)\n","example_input = example_input.to(device)\n","traced_script_module = torch.jit.script(model, example_input)\n","traced_script_module.save(f'model_epoch_{epoch+1}_script.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir tensorboard_path"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713684839966,"user":{"displayName":"Jiaxuan Li","userId":"07141872132042446991"},"user_tz":-480},"id":"3LfRvpDqh0h7","outputId":"46afcc98-552c-48b4-c57d-18986a0abb62"},"outputs":[],"source":["# for name, param in model.named_parameters():\n","#     print(f\"{name}: requires_grad = {param.requires_grad}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdlOnUpBKs68"},"outputs":[],"source":["# import h5py\n","# import torch\n","\n","# def save_model_weights_to_hdf5(model, filepath):\n","#     with h5py.File(filepath, 'w') as f:\n","#         for name, param in model.named_parameters():\n","#             param_value = param.data.numpy()\n","#             f.create_dataset(name, data=param_value)\n","\n","# def load_model_weights_from_hdf5(model, filepath):\n","\n","\n","#     with h5py.File(filepath, 'r') as f:\n","#         for name in f.keys():\n","#             param_value = torch.tensor(f[name][:])\n","#             model._parameters[name] = torch.nn.Parameter(param_value)\n","\n","\n","# save_model_weights_to_hdf5(model, '/content/drive/MyDrive/project_slam/model_weights.h5')\n","\n","# load_model_weights_from_hdf5(model, 'model_weights.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cr0Hu_lV4pH2"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
